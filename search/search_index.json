{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTabular What is it? PyTabular (python-tabular in pypi ) is a python package that allows for programmatic execution on your tabular models! This is possible thanks to Pythonnet and Microsoft's .Net APIs on Azure Analysis Services . Currently, this build is tested and working on Windows Operating System only. Help is needed to expand this for other operating systems. The package should have the dll files included when you import it. See Documentation Here . PyTabular is still considered alpha while I'm working on building out the proper tests and testing environments, so I can ensure some kind of stability in features. Please send bugs my way! Preferably in the issues section in Github. I want to harden this project so many can use it easily. I currently have local pytest for python 3.6 to 3.10 and run those tests through a local AAS and Gen2 model. Getting Started See the Pypi project for available versions. To become PEP8 compliant with naming conventions, serious name changes were made in 0.3.5. Instal v. 0.3.4 or lower to get the older naming conventions. python3 -m pip install python-tabular #install specific version python3 -m pip install python-tabular==0.3.4 In your python environment, import pytabular and call the main Tabular Class. Only parameter needed is a solid connection string. import pytabular model = pytabular.Tabular(CONNECTION_STR) I'm a big fan of logging, if you don't want any just get the logger and disable it. import pytabular pytabular.logger.disabled = True You can query your models with the Query method from your tabular class. For Dax Queries, it will need the full Dax syntax. See EVALUATE example . This will return a Pandas DataFrame . If you are looking to return a single value, see below. Simply wrap your query in the the curly brackets. The method will take that single cell table and just return the individual value. You can also query your DMV. See below for example. See PyTabular Docs for Query . #Run basic queries DAX_QUERY = \"EVALUATE TOPN(100, 'Table1')\" model.query(DAX_QUERY) #returns pd.DataFrame() #or... DMV_QUERY = \"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\" model.query(DMV_QUERY) #returns pd.DataFrame() #or... SINGLE_VALUE_QUERY_EX = \"EVALUATE {1}\" model.query(SINGLE_VALUE_QUERY_EX) #returns 1 #or... FILE_PATH = 'C:\\\\FILEPATHEXAMPLE\\\\file.dax' #or file.txt model.query(FILE_PATH) #Will return same logic as above, single values if possible else will return pd.DataFrame() You can also explore your tables, partitions, and columns. Via the Attributes from your Tabular class. #Explore tables... dir(model.Tables['Table Name']) #Explore columns & partitions dir(model.Tables['Table Name'].Partitions['Partition Name']) #Only a few features right now, but check out the built in methods. model.Tables['Table Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].last_refresh() #or model.Tables['Table Name'].row_count() #or model.Tables['Table Name'].Columns['Column Name'].distinct_count() Refresh method to handle refreshes on your model. This is synchronous. Should be flexible enough to handle a variety of inputs. See PyTabular Docs for Refreshing Tables and Partitions . Most basic way to refresh is input the table name string. The method will search for table and output exeption if unable to find it. For partitions you will need a key, value combination. Example, {'Table1':'Partition1'} . You can also take the key value pair and iterate through a group of partitions. Example, {'Table1':['Partition1','Partition2']} . Rather than providing a string, you can also input the actual class. See below for those examples, and you can acess them from the built in attributes self.Tables , self.Partitions or explore through the .Net classes yourself in self.Model.Tables . #You have a few options when refreshing. model.refresh('Table Name') #or... model.refresh(['Table1','Table2','Table3']) #or... model.refresh(<Table Class>) #or... model.refresh(<Partition Class>) #or... model.refresh({'Table Name':'Partition Name'}) #or any kind of weird combination like model.refresh([{<Table Class>:<Partition Class>,'Table Name':['Partition1','Partition2']},'Table Name','Table Name2']) #You can even run through the Tables & Partition Attributes model.Tables['Table Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].refresh() #Default Tracing happens automatically, but can be removed by... model.refresh(['Table1','Table2'], trace = None) It's not uncommon to need to run through some checks on specific Tables, Partitions, Columns, Etc... #Get Row Count from model model.Tables['Table Name'].row_count() #Get Last Refresh time from a partition model.Tables['Table Name'].last_refresh() #Get Distinct Count or Values from a Column model.Tables['Table Name'].Columns['Column Name'].distinct_count() #or model.Tables['Table Name'].Columns['Column Name'].values() Use Cases If blank table, then refresh table. This will use the function Return_Zero_Row_Tables and the method Refresh from the Tabular class. import pytabular model = pytabular.Tabular(CONNECTION_STR) tables = model.Tables.find_zero_rows() if len(tables) > 0: model.refresh(tables) Sneak in a refresh. This will use the method Is_Process and the method Refresh from the Tabular class. It will check the DMV to see if any jobs are currently running classified as processing. import pytabular model = pytabular.Tabular(CONNECTION_STR) if model.is_process(): #do what you want if there is a refresh happening else: model.refresh(TABLES_OR_PARTITIONS_TO_REFRESH) Show refresh times in model. This will use the function Table_Last_Refresh_Times and the method Create_Table from the Tabular class. It will search through the model for all tables and partitions and pull the 'RefreshedTime' property from it. It will return results into a pandas dataframe, which will then be converted into an M expression used for a new table. import pytabular model = pytabular.Tabular(CONNECTION_STR) df = model.Tables.last_refresh() model.create_table(df, 'Refresh Times') If BPA Violation, then revert deployment. Uses a few things. First the BPA Class , then the TE2 Class , and will finish with the Analyze_BPA method. Did not want to re-invent the wheel with the amazing work done with Tabular Editor and it's BPA capabilities. import pytabular model = pytabular.Tabular(CONNECTION_STR) te2 = pytabular.TabularEditor() #Feel free to input your TE2 File path or this will download for you. bpa = pytabular.BPA() #Fee free to input your own BPA file or this will download for you from: https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json results = model.analyze_bpa(te2.exe,bpa.location) if len(results) > 0: #Revert deployment here! Loop through and query Dax files Let's say you have multiple dax queries you would like to store and run through as checks. The Query method on the Tabular class can also take file paths. Can really be any file type as it's just checking os.path.isfile(). But would suggest .dax or .txt. It will read the file that use that as the new Query_str argument. import pytabular model = pytabular.Tabular(CONNECTION_STR) LIST_OF_FILE_PATHS = ['C:\\\\FilePath\\\\file1.dax','C:\\\\FilePath\\\\file1.txt','C:\\\\FilePath\\\\file2.dax','C:\\\\FilePath\\\\file2.txt'] for file_path in LIST_OF_FILE_PATHS: model.query(file_path) Advanced Refreshing with Pre and Post Checks Maybe you are introducing new logic to a fact table, and you need to ensure that a measure checking last month values never changes. To do that you can take advantage of the Refresh_Check and Refresh_Check_Collection classes (Sorry, I know the documentation stinks right now). But using those you can build out something that would first check the results of the measure, then refresh, then check the results of the measure after refresh, and lastly perform your desired check. In this case the pre value matches the post value. When refreshing and your pre does not equal post, it would fail and give an assertion error in your logging. from pytabular import Tabular from pytabular.refresh import RefreshCheck, RefreshCheckCollection model = Tabular(CONNECTION_STR) # This is our custom check that we want to run after refresh. # Does the pre refresh value match the post refresh value. def sum_of_sales_assertion(pre, post): return pre == post # This is where we put it all together into the `Refresh_Check` class. Give it a name, give it a query to run, and give it the assertion you want to make. sum_of_last_month_sales = RefreshCheck( 'Last Month Sales', lambda: model.query(\"EVALUATE {[Last Month Sales]}\") ,sum_of_sales_assertion ) # Here we are adding it to a `Refresh_Check_Collection` because you can have more than on `Refresh_Check` to run. all_refresh_check = RefreshCheckCollection([sum_of_last_month_sales]) model.Refresh( 'Fact Table Name', refresh_checks = RefreshCheckCollection([sum_of_last_month_sales]) ) Query as Another User There are plenty of tools that allow you to query as an 'Effective User' inheriting their security when querying. This is an extremely valuable concept built natively into the .Net apis. My only gripe is they were all UI based. This allows you to programmatically connect as an effective user and query in Python. You could easily loop through all your users to run tests on their security. import pytabular as p #Connect to your model like usual... model = p.Tabular(CONNECTION_STR) #This will be the query I run... query_str = ''' EVALUATE SUMMARIZE( 'Product Dimension', 'Product Dimension'[Product Name], \"Total Product Sales\", [Total Sales] ) ''' #This will be the user I want to query as... user_email = 'user1@company.com' #Base line, to query as the user connecting to the model. model.query(query_str) #Option 1, Connect via connection class... user1 = p.Connection(model.Server, Effective_User = user_email) user1.query(query_str) #Option 2, Just add Effective_User model.query(query_str, Effective_User = user_email) #PyTabular will do it's best to handle multiple accounts... #So you won't have to reconnect on every query Refresh Related Tables Ever need to refresh related tables of a Fact? Now should be a lot easier. import pytabular as p #Connect to model model = p.Tabular(CONNECTION_STR) #Get related tables tables = model.Tables[TABLE_NAME].related() #Now just refresh like usual... tables.refresh() Documenting a Model The Tabular model contains a lot of information that can be used to generation documentation if filled in. Currently the markdown files are generated with the Docusaurs heading in place, but this will be changed in future to support multiple documentation platforms. Tip : With Tabular Editor 2 (Free) or 3 (Paid) you can easily add Descriptioms, Translations (Cultures) and other additonal information that can later be used for generating the documentation. Args: - model : Tabular - friendly_name : Default > No Value To specify the location of the docs, just supply the save location with a new folder name argument. - save_location : Default > docs Each page in the generation process has it's own specific name, with these arguments you can rename them to your liking. - general_page_url : Default > 1-general-information.md - measure_page_url : Default > 2-measures.md - table_page_url : Default > 3-tables.md - column_page_url : Default > 4-columns.md - roles_page_url : Default > 5-roles.md Documenting a Model The simpelst way to document a tabular model is to connect to the model, and initialize the documentation and execute save_documentation() . import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs docs = pytabular.ModelDocumenter(model) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation() Documenting a Model with Cultures Some model creators choose to add cultures to a tabular model for different kinds of reasons. We can leverage those cultures to use the translation names instead of the original object names. In order to this you can set translations to True and specify the culture you want to use (e.g. 'en-US' ). import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs docs = pytabular.ModelDocumenter(model) # Set the translation for documentation to an available culture. # By setting the Tranlsations to `True` it will check if it exists and if it does, # it will start using the translations for the docs docs.set_translations( enable_translations = True, culture = 'en-US' ) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation() Documenting a Power BI > Local Model. The Local model doesn't have a \"name\", only an Id. So we need to Supply a \"Friendly Name\", which will be used to store the markdown files. import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs and set a friendly name to store the markdown files. docs = pytabular.ModelDocumenter( model = model, friendly_name = \"Adventure Works\" ) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation() Contributing See CONTRIBUTING.md","title":"Home"},{"location":"#pytabular","text":"","title":"PyTabular"},{"location":"#what-is-it","text":"PyTabular (python-tabular in pypi ) is a python package that allows for programmatic execution on your tabular models! This is possible thanks to Pythonnet and Microsoft's .Net APIs on Azure Analysis Services . Currently, this build is tested and working on Windows Operating System only. Help is needed to expand this for other operating systems. The package should have the dll files included when you import it. See Documentation Here . PyTabular is still considered alpha while I'm working on building out the proper tests and testing environments, so I can ensure some kind of stability in features. Please send bugs my way! Preferably in the issues section in Github. I want to harden this project so many can use it easily. I currently have local pytest for python 3.6 to 3.10 and run those tests through a local AAS and Gen2 model.","title":"What is it?"},{"location":"#getting-started","text":"See the Pypi project for available versions. To become PEP8 compliant with naming conventions, serious name changes were made in 0.3.5. Instal v. 0.3.4 or lower to get the older naming conventions. python3 -m pip install python-tabular #install specific version python3 -m pip install python-tabular==0.3.4 In your python environment, import pytabular and call the main Tabular Class. Only parameter needed is a solid connection string. import pytabular model = pytabular.Tabular(CONNECTION_STR) I'm a big fan of logging, if you don't want any just get the logger and disable it. import pytabular pytabular.logger.disabled = True You can query your models with the Query method from your tabular class. For Dax Queries, it will need the full Dax syntax. See EVALUATE example . This will return a Pandas DataFrame . If you are looking to return a single value, see below. Simply wrap your query in the the curly brackets. The method will take that single cell table and just return the individual value. You can also query your DMV. See below for example. See PyTabular Docs for Query . #Run basic queries DAX_QUERY = \"EVALUATE TOPN(100, 'Table1')\" model.query(DAX_QUERY) #returns pd.DataFrame() #or... DMV_QUERY = \"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\" model.query(DMV_QUERY) #returns pd.DataFrame() #or... SINGLE_VALUE_QUERY_EX = \"EVALUATE {1}\" model.query(SINGLE_VALUE_QUERY_EX) #returns 1 #or... FILE_PATH = 'C:\\\\FILEPATHEXAMPLE\\\\file.dax' #or file.txt model.query(FILE_PATH) #Will return same logic as above, single values if possible else will return pd.DataFrame() You can also explore your tables, partitions, and columns. Via the Attributes from your Tabular class. #Explore tables... dir(model.Tables['Table Name']) #Explore columns & partitions dir(model.Tables['Table Name'].Partitions['Partition Name']) #Only a few features right now, but check out the built in methods. model.Tables['Table Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].last_refresh() #or model.Tables['Table Name'].row_count() #or model.Tables['Table Name'].Columns['Column Name'].distinct_count() Refresh method to handle refreshes on your model. This is synchronous. Should be flexible enough to handle a variety of inputs. See PyTabular Docs for Refreshing Tables and Partitions . Most basic way to refresh is input the table name string. The method will search for table and output exeption if unable to find it. For partitions you will need a key, value combination. Example, {'Table1':'Partition1'} . You can also take the key value pair and iterate through a group of partitions. Example, {'Table1':['Partition1','Partition2']} . Rather than providing a string, you can also input the actual class. See below for those examples, and you can acess them from the built in attributes self.Tables , self.Partitions or explore through the .Net classes yourself in self.Model.Tables . #You have a few options when refreshing. model.refresh('Table Name') #or... model.refresh(['Table1','Table2','Table3']) #or... model.refresh(<Table Class>) #or... model.refresh(<Partition Class>) #or... model.refresh({'Table Name':'Partition Name'}) #or any kind of weird combination like model.refresh([{<Table Class>:<Partition Class>,'Table Name':['Partition1','Partition2']},'Table Name','Table Name2']) #You can even run through the Tables & Partition Attributes model.Tables['Table Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].refresh() #Default Tracing happens automatically, but can be removed by... model.refresh(['Table1','Table2'], trace = None) It's not uncommon to need to run through some checks on specific Tables, Partitions, Columns, Etc... #Get Row Count from model model.Tables['Table Name'].row_count() #Get Last Refresh time from a partition model.Tables['Table Name'].last_refresh() #Get Distinct Count or Values from a Column model.Tables['Table Name'].Columns['Column Name'].distinct_count() #or model.Tables['Table Name'].Columns['Column Name'].values()","title":"Getting Started"},{"location":"#use-cases","text":"","title":"Use Cases"},{"location":"#if-blank-table-then-refresh-table","text":"This will use the function Return_Zero_Row_Tables and the method Refresh from the Tabular class. import pytabular model = pytabular.Tabular(CONNECTION_STR) tables = model.Tables.find_zero_rows() if len(tables) > 0: model.refresh(tables)","title":"If blank table, then refresh table."},{"location":"#sneak-in-a-refresh","text":"This will use the method Is_Process and the method Refresh from the Tabular class. It will check the DMV to see if any jobs are currently running classified as processing. import pytabular model = pytabular.Tabular(CONNECTION_STR) if model.is_process(): #do what you want if there is a refresh happening else: model.refresh(TABLES_OR_PARTITIONS_TO_REFRESH)","title":"Sneak in a refresh."},{"location":"#show-refresh-times-in-model","text":"This will use the function Table_Last_Refresh_Times and the method Create_Table from the Tabular class. It will search through the model for all tables and partitions and pull the 'RefreshedTime' property from it. It will return results into a pandas dataframe, which will then be converted into an M expression used for a new table. import pytabular model = pytabular.Tabular(CONNECTION_STR) df = model.Tables.last_refresh() model.create_table(df, 'Refresh Times')","title":"Show refresh times in model."},{"location":"#if-bpa-violation-then-revert-deployment","text":"Uses a few things. First the BPA Class , then the TE2 Class , and will finish with the Analyze_BPA method. Did not want to re-invent the wheel with the amazing work done with Tabular Editor and it's BPA capabilities. import pytabular model = pytabular.Tabular(CONNECTION_STR) te2 = pytabular.TabularEditor() #Feel free to input your TE2 File path or this will download for you. bpa = pytabular.BPA() #Fee free to input your own BPA file or this will download for you from: https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json results = model.analyze_bpa(te2.exe,bpa.location) if len(results) > 0: #Revert deployment here!","title":"If BPA Violation, then revert deployment."},{"location":"#loop-through-and-query-dax-files","text":"Let's say you have multiple dax queries you would like to store and run through as checks. The Query method on the Tabular class can also take file paths. Can really be any file type as it's just checking os.path.isfile(). But would suggest .dax or .txt. It will read the file that use that as the new Query_str argument. import pytabular model = pytabular.Tabular(CONNECTION_STR) LIST_OF_FILE_PATHS = ['C:\\\\FilePath\\\\file1.dax','C:\\\\FilePath\\\\file1.txt','C:\\\\FilePath\\\\file2.dax','C:\\\\FilePath\\\\file2.txt'] for file_path in LIST_OF_FILE_PATHS: model.query(file_path)","title":"Loop through and query Dax files"},{"location":"#advanced-refreshing-with-pre-and-post-checks","text":"Maybe you are introducing new logic to a fact table, and you need to ensure that a measure checking last month values never changes. To do that you can take advantage of the Refresh_Check and Refresh_Check_Collection classes (Sorry, I know the documentation stinks right now). But using those you can build out something that would first check the results of the measure, then refresh, then check the results of the measure after refresh, and lastly perform your desired check. In this case the pre value matches the post value. When refreshing and your pre does not equal post, it would fail and give an assertion error in your logging. from pytabular import Tabular from pytabular.refresh import RefreshCheck, RefreshCheckCollection model = Tabular(CONNECTION_STR) # This is our custom check that we want to run after refresh. # Does the pre refresh value match the post refresh value. def sum_of_sales_assertion(pre, post): return pre == post # This is where we put it all together into the `Refresh_Check` class. Give it a name, give it a query to run, and give it the assertion you want to make. sum_of_last_month_sales = RefreshCheck( 'Last Month Sales', lambda: model.query(\"EVALUATE {[Last Month Sales]}\") ,sum_of_sales_assertion ) # Here we are adding it to a `Refresh_Check_Collection` because you can have more than on `Refresh_Check` to run. all_refresh_check = RefreshCheckCollection([sum_of_last_month_sales]) model.Refresh( 'Fact Table Name', refresh_checks = RefreshCheckCollection([sum_of_last_month_sales]) )","title":"Advanced Refreshing with Pre and Post Checks"},{"location":"#query-as-another-user","text":"There are plenty of tools that allow you to query as an 'Effective User' inheriting their security when querying. This is an extremely valuable concept built natively into the .Net apis. My only gripe is they were all UI based. This allows you to programmatically connect as an effective user and query in Python. You could easily loop through all your users to run tests on their security. import pytabular as p #Connect to your model like usual... model = p.Tabular(CONNECTION_STR) #This will be the query I run... query_str = ''' EVALUATE SUMMARIZE( 'Product Dimension', 'Product Dimension'[Product Name], \"Total Product Sales\", [Total Sales] ) ''' #This will be the user I want to query as... user_email = 'user1@company.com' #Base line, to query as the user connecting to the model. model.query(query_str) #Option 1, Connect via connection class... user1 = p.Connection(model.Server, Effective_User = user_email) user1.query(query_str) #Option 2, Just add Effective_User model.query(query_str, Effective_User = user_email) #PyTabular will do it's best to handle multiple accounts... #So you won't have to reconnect on every query","title":"Query as Another User"},{"location":"#refresh-related-tables","text":"Ever need to refresh related tables of a Fact? Now should be a lot easier. import pytabular as p #Connect to model model = p.Tabular(CONNECTION_STR) #Get related tables tables = model.Tables[TABLE_NAME].related() #Now just refresh like usual... tables.refresh()","title":"Refresh Related Tables"},{"location":"#documenting-a-model","text":"The Tabular model contains a lot of information that can be used to generation documentation if filled in. Currently the markdown files are generated with the Docusaurs heading in place, but this will be changed in future to support multiple documentation platforms. Tip : With Tabular Editor 2 (Free) or 3 (Paid) you can easily add Descriptioms, Translations (Cultures) and other additonal information that can later be used for generating the documentation. Args: - model : Tabular - friendly_name : Default > No Value To specify the location of the docs, just supply the save location with a new folder name argument. - save_location : Default > docs Each page in the generation process has it's own specific name, with these arguments you can rename them to your liking. - general_page_url : Default > 1-general-information.md - measure_page_url : Default > 2-measures.md - table_page_url : Default > 3-tables.md - column_page_url : Default > 4-columns.md - roles_page_url : Default > 5-roles.md","title":"Documenting a Model"},{"location":"#documenting-a-model_1","text":"The simpelst way to document a tabular model is to connect to the model, and initialize the documentation and execute save_documentation() . import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs docs = pytabular.ModelDocumenter(model) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation()","title":"Documenting a Model"},{"location":"#documenting-a-model-with-cultures","text":"Some model creators choose to add cultures to a tabular model for different kinds of reasons. We can leverage those cultures to use the translation names instead of the original object names. In order to this you can set translations to True and specify the culture you want to use (e.g. 'en-US' ). import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs docs = pytabular.ModelDocumenter(model) # Set the translation for documentation to an available culture. # By setting the Tranlsations to `True` it will check if it exists and if it does, # it will start using the translations for the docs docs.set_translations( enable_translations = True, culture = 'en-US' ) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation()","title":"Documenting a Model with Cultures"},{"location":"#documenting-a-power-bi-local-model","text":"The Local model doesn't have a \"name\", only an Id. So we need to Supply a \"Friendly Name\", which will be used to store the markdown files. import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs and set a friendly name to store the markdown files. docs = pytabular.ModelDocumenter( model = model, friendly_name = \"Adventure Works\" ) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation()","title":"Documenting a Power BI &gt; Local Model."},{"location":"#contributing","text":"See CONTRIBUTING.md","title":"Contributing"},{"location":"Best%20Practice%20Analyzer/","text":"BPA source BPA( file_path: str = 'Default' ) Setting BPA Class for future work... download_bpa_file source .download_bpa_file( download_location: str = 'https: //raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json', folder: str = 'Best_Practice_Analyzer', auto_remove = True ) Runs a request.get() to retrieve the json file from web. Will return and store in directory. Will also register the removal of the new directory and file when exiting program. Args Download_Location ( type , optional) : F. Defaults to [Microsoft GitHub BPA]'https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json'. Folder (str, optional) : New Folder String. Defaults to 'Best_Practice_Analyzer'. Auto_Remove (bool, optional) : If you wish to Auto Remove when script exits. Defaults to True. Returns str : File Path for the newly downloaded BPA.","title":"Best Practice Analyzer"},{"location":"Best%20Practice%20Analyzer/#_1","text":"","title":""},{"location":"Best%20Practice%20Analyzer/#bpa","text":"source BPA( file_path: str = 'Default' ) Setting BPA Class for future work...","title":"BPA"},{"location":"Best%20Practice%20Analyzer/#download_bpa_file","text":"source .download_bpa_file( download_location: str = 'https: //raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json', folder: str = 'Best_Practice_Analyzer', auto_remove = True ) Runs a request.get() to retrieve the json file from web. Will return and store in directory. Will also register the removal of the new directory and file when exiting program. Args Download_Location ( type , optional) : F. Defaults to [Microsoft GitHub BPA]'https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json'. Folder (str, optional) : New Folder String. Defaults to 'Best_Practice_Analyzer'. Auto_Remove (bool, optional) : If you wish to Auto Remove when script exits. Defaults to True. Returns str : File Path for the newly downloaded BPA.","title":"download_bpa_file"},{"location":"Column/","text":"PyColumn source PyColumn( object, table ) Wrapper for Column . With a few other bells and whistles added to it. Notice the PyObject magic method __getattr__() will search in self._object if it is unable to find it in the default attributes. This let's you also easily check the default .Net properties. Args Table : Parent Table to the Column Methods: .get_dependencies source .get_dependencies() Returns the dependant columns of a measure .get_sample_values source .get_sample_values( top_n: int = 3 ) Get sample values of column. .distinct_count source .distinct_count( no_blank = False ) Get DISTINCTCOUNT of Column. Args no_blank (bool, optional) : Ability to call DISTINCTCOUNTNOBLANK . Defaults to False. Returns int : Number of Distinct Count from column. If no_blank == True then will return number of Distinct Count no blanks. .values source .values() Get single column DataFrame of VALUES Returns DataFrame : Single Column DataFrame of Values. PyColumns source PyColumns( objects ) Groups together multiple columns. See PyObjects class for what more it can do. You can interact with PyColumns straight from model. For ex: model.Columns . Or through individual tables model.Tables[TABLE_NAME].Columns . You can even filter down with .Find() . For example find all columns with Key in name. model.Columns.Find('Key') . Methods: .query_all source .query_all( query_function: str = 'COUNTROWS(VALUES(_))' ) This will dynamically create a query to pull all columns from the model and run the query function. It will replace the _ with the column to run. Args query_function (str, optional) : Dax query is dynamically building a query with the UNION & ROW DAX Functions. Returns DataFrame : Returns dataframe with results.","title":"Column"},{"location":"Column/#_1","text":"","title":""},{"location":"Column/#pycolumn","text":"source PyColumn( object, table ) Wrapper for Column . With a few other bells and whistles added to it. Notice the PyObject magic method __getattr__() will search in self._object if it is unable to find it in the default attributes. This let's you also easily check the default .Net properties. Args Table : Parent Table to the Column Methods:","title":"PyColumn"},{"location":"Column/#get_dependencies","text":"source .get_dependencies() Returns the dependant columns of a measure","title":".get_dependencies"},{"location":"Column/#get_sample_values","text":"source .get_sample_values( top_n: int = 3 ) Get sample values of column.","title":".get_sample_values"},{"location":"Column/#distinct_count","text":"source .distinct_count( no_blank = False ) Get DISTINCTCOUNT of Column. Args no_blank (bool, optional) : Ability to call DISTINCTCOUNTNOBLANK . Defaults to False. Returns int : Number of Distinct Count from column. If no_blank == True then will return number of Distinct Count no blanks.","title":".distinct_count"},{"location":"Column/#values","text":"source .values() Get single column DataFrame of VALUES Returns DataFrame : Single Column DataFrame of Values.","title":".values"},{"location":"Column/#pycolumns","text":"source PyColumns( objects ) Groups together multiple columns. See PyObjects class for what more it can do. You can interact with PyColumns straight from model. For ex: model.Columns . Or through individual tables model.Tables[TABLE_NAME].Columns . You can even filter down with .Find() . For example find all columns with Key in name. model.Columns.Find('Key') . Methods:","title":"PyColumns"},{"location":"Column/#query_all","text":"source .query_all( query_function: str = 'COUNTROWS(VALUES(_))' ) This will dynamically create a query to pull all columns from the model and run the query function. It will replace the _ with the column to run. Args query_function (str, optional) : Dax query is dynamically building a query with the UNION & ROW DAX Functions. Returns DataFrame : Returns dataframe with results.","title":".query_all"},{"location":"Logic%20Utils/","text":"ticks_to_datetime source .ticks_to_datetime( ticks: int ) Converts a C# System DateTime Tick into a Python DateTime Args ticks (int) : C# DateTime Tick Returns datetime : datetime.datetime pandas_datatype_to_tabular_datatype source .pandas_datatype_to_tabular_datatype( df: pd.DataFrame ) WiP takes dataframe columns and gets respective tabular column datatype. ( NumPy Datatypes and Tabular Datatypes ) Args df (pd.DataFrame) : Pandas DataFrame Returns Dict : EX {'col1': , 'col2': , 'col3': } pd_dataframe_to_m_expression source .pd_dataframe_to_m_expression( df: pd.DataFrame ) This will take a pandas dataframe and convert to an m expression For example this DF: col1 col2 0 1 3 1 2 4 | | V Will convert to this expression string: let Source=#table({\"col1\",\"col2\"}, { {\"1\",\"3\"},{\"2\",\"4\"} }) in Source Args df (pd.DataFrame) : Pandas DataFrame Returns str : Currently only returning string values in your tabular model. remove_folder_and_contents source .remove_folder_and_contents( folder_location ) Internal used in tabular_editor.py and best_practice_analyzer.py. Args folder_location (str) : Folder path to remove directory and contents. remove_suffix source .remove_suffix( input_string, suffix ) Adding for >3.9 compatiblity. Stackoverflow Answer Args input_string (str) : input string to remove suffix from suffix (str) : suffix to be removed Returns str : input_str with suffix removed remove_file source .remove_file( file_path ) Just os.remove() but wanted a logger.info() with it. Args file_path : See os.remove get_sub_list source .get_sub_list( lst: list, n: int ) Nest list by n amount... get_sub_list([1,2,3,4,5,6],2) == [[1,2],[3,4],[5,6]] Args lst (list) : List to nest. n (int) : Amount to nest list. Returns list : Nested list. get_value_to_df source .get_value_to_df( query: AdomdDataReader, index: int ) Gets the values from the AdomdDataReader to convert the .Net Object into a tangible python value to work with in pandas. Lots of room for improvement on this one. Args Query (AdomdDataReader) : AdomdDataReader index (int) : Index of the value to perform the logic on. dataframe_to_dict source .dataframe_to_dict( df: pd.DataFrame ) Convert to Dataframe to dictionary and alter columns names with; - Underscores (_) to spaces - All Strings are converted to Title Case. Args df (pd.DataFrame) : Original table that needs to be converted to a list with dicts. Returns list of dictionaries. dict_to_markdown_table source .dict_to_markdown_table( list_of_dicts: list, columns_to_include: list = None ) Description: Generate a Markdown table based on a list of dictionaries. Args list_of_dicts (list) : List of Dictionaries that need to be converted to a markdown table. columns_to_include (list) : Default = None, and all colums are included. If a list is supplied, those columns will be included. Returns String that will represent a table in Markdown. Example columns = ['Referenced Object Type', 'Referenced Table', 'Referenced Object'] dict_to_markdown_table(dependancies, columns) Result: | Referenced Object Type | Referenced Table | Referenced Object | | ---------------------- | ---------------- | ------------------------------- | | TABLE | Cases | Cases | | COLUMN | Cases | IsClosed | | CALC_COLUMN | Cases | Resolution Time (Working Hours) |","title":"Logic Utils"},{"location":"Logic%20Utils/#_1","text":"","title":""},{"location":"Logic%20Utils/#ticks_to_datetime","text":"source .ticks_to_datetime( ticks: int ) Converts a C# System DateTime Tick into a Python DateTime Args ticks (int) : C# DateTime Tick Returns datetime : datetime.datetime","title":"ticks_to_datetime"},{"location":"Logic%20Utils/#pandas_datatype_to_tabular_datatype","text":"source .pandas_datatype_to_tabular_datatype( df: pd.DataFrame ) WiP takes dataframe columns and gets respective tabular column datatype. ( NumPy Datatypes and Tabular Datatypes ) Args df (pd.DataFrame) : Pandas DataFrame Returns Dict : EX {'col1': , 'col2': , 'col3': }","title":"pandas_datatype_to_tabular_datatype"},{"location":"Logic%20Utils/#pd_dataframe_to_m_expression","text":"source .pd_dataframe_to_m_expression( df: pd.DataFrame ) This will take a pandas dataframe and convert to an m expression For example this DF: col1 col2 0 1 3 1 2 4 | | V Will convert to this expression string: let Source=#table({\"col1\",\"col2\"}, { {\"1\",\"3\"},{\"2\",\"4\"} }) in Source Args df (pd.DataFrame) : Pandas DataFrame Returns str : Currently only returning string values in your tabular model.","title":"pd_dataframe_to_m_expression"},{"location":"Logic%20Utils/#remove_folder_and_contents","text":"source .remove_folder_and_contents( folder_location ) Internal used in tabular_editor.py and best_practice_analyzer.py. Args folder_location (str) : Folder path to remove directory and contents.","title":"remove_folder_and_contents"},{"location":"Logic%20Utils/#remove_suffix","text":"source .remove_suffix( input_string, suffix ) Adding for >3.9 compatiblity. Stackoverflow Answer Args input_string (str) : input string to remove suffix from suffix (str) : suffix to be removed Returns str : input_str with suffix removed","title":"remove_suffix"},{"location":"Logic%20Utils/#remove_file","text":"source .remove_file( file_path ) Just os.remove() but wanted a logger.info() with it. Args file_path : See os.remove","title":"remove_file"},{"location":"Logic%20Utils/#get_sub_list","text":"source .get_sub_list( lst: list, n: int ) Nest list by n amount... get_sub_list([1,2,3,4,5,6],2) == [[1,2],[3,4],[5,6]] Args lst (list) : List to nest. n (int) : Amount to nest list. Returns list : Nested list.","title":"get_sub_list"},{"location":"Logic%20Utils/#get_value_to_df","text":"source .get_value_to_df( query: AdomdDataReader, index: int ) Gets the values from the AdomdDataReader to convert the .Net Object into a tangible python value to work with in pandas. Lots of room for improvement on this one. Args Query (AdomdDataReader) : AdomdDataReader index (int) : Index of the value to perform the logic on.","title":"get_value_to_df"},{"location":"Logic%20Utils/#dataframe_to_dict","text":"source .dataframe_to_dict( df: pd.DataFrame ) Convert to Dataframe to dictionary and alter columns names with; - Underscores (_) to spaces - All Strings are converted to Title Case. Args df (pd.DataFrame) : Original table that needs to be converted to a list with dicts. Returns list of dictionaries.","title":"dataframe_to_dict"},{"location":"Logic%20Utils/#dict_to_markdown_table","text":"source .dict_to_markdown_table( list_of_dicts: list, columns_to_include: list = None ) Description: Generate a Markdown table based on a list of dictionaries. Args list_of_dicts (list) : List of Dictionaries that need to be converted to a markdown table. columns_to_include (list) : Default = None, and all colums are included. If a list is supplied, those columns will be included. Returns String that will represent a table in Markdown. Example columns = ['Referenced Object Type', 'Referenced Table', 'Referenced Object'] dict_to_markdown_table(dependancies, columns) Result: | Referenced Object Type | Referenced Table | Referenced Object | | ---------------------- | ---------------- | ------------------------------- | | TABLE | Cases | Cases | | COLUMN | Cases | IsClosed | | CALC_COLUMN | Cases | Resolution Time (Working Hours) |","title":"dict_to_markdown_table"},{"location":"PBI%20Helper/","text":"find_local_pbi_instances source .find_local_pbi_instances() The real genius is from this Dax Studio PowerBIHelper . I just wanted it in python not C#, so reverse engineered what DaxStudio did. It will run some powershell scripts to pull the appropriate info. Then will spit out a list with tuples inside. You can use the connection string to connect to your model with pytabular. Returns list : Example - [('PBI File Name1','localhost:{port}'),('PBI File Name2','localhost:{port}')]","title":"PBI Helper"},{"location":"PBI%20Helper/#_1","text":"","title":""},{"location":"PBI%20Helper/#find_local_pbi_instances","text":"source .find_local_pbi_instances() The real genius is from this Dax Studio PowerBIHelper . I just wanted it in python not C#, so reverse engineered what DaxStudio did. It will run some powershell scripts to pull the appropriate info. Then will spit out a list with tuples inside. You can use the connection string to connect to your model with pytabular. Returns list : Example - [('PBI File Name1','localhost:{port}'),('PBI File Name2','localhost:{port}')]","title":"find_local_pbi_instances"},{"location":"Partition/","text":"PyPartition source PyPartition( object, table ) Wrapper for Partition . With a few other bells and whistles added to it. Args Table : Parent Table to the Column Methods: .last_refresh source .last_refresh() Queries RefreshedTime attribute in the partition and converts from C# Ticks to Python datetime Returns datetime : Last Refreshed time of Partition in datetime format .refresh source .refresh( *args, **kwargs ) Same method from Model Refresh, you can pass through any extra parameters. For example: Tabular().Tables['Table Name'].Partitions[0].refresh() Returns DataFrame : Returns pandas dataframe with some refresh details PyPartitions source PyPartitions( objects ) Groups together multiple partitions. See PyObjects class for what more it can do. You can interact with PyPartitions straight from model. For ex: model.Partitions . Or through individual tables model.Tables[TABLE_NAME].Partitions . You can even filter down with .Find() . For example find all partition with prev-year in name. model.Partitions.Find('prev-year') .","title":"Partition"},{"location":"Partition/#_1","text":"","title":""},{"location":"Partition/#pypartition","text":"source PyPartition( object, table ) Wrapper for Partition . With a few other bells and whistles added to it. Args Table : Parent Table to the Column Methods:","title":"PyPartition"},{"location":"Partition/#last_refresh","text":"source .last_refresh() Queries RefreshedTime attribute in the partition and converts from C# Ticks to Python datetime Returns datetime : Last Refreshed time of Partition in datetime format","title":".last_refresh"},{"location":"Partition/#refresh","text":"source .refresh( *args, **kwargs ) Same method from Model Refresh, you can pass through any extra parameters. For example: Tabular().Tables['Table Name'].Partitions[0].refresh() Returns DataFrame : Returns pandas dataframe with some refresh details","title":".refresh"},{"location":"Partition/#pypartitions","text":"source PyPartitions( objects ) Groups together multiple partitions. See PyObjects class for what more it can do. You can interact with PyPartitions straight from model. For ex: model.Partitions . Or through individual tables model.Tables[TABLE_NAME].Partitions . You can even filter down with .Find() . For example find all partition with prev-year in name. model.Partitions.Find('prev-year') .","title":"PyPartitions"},{"location":"Queries/","text":"Connection source Connection( server, effective_user = None ) Subclass for Adomdclient . With some extra items on top. Right now designed for internal use. For example, Query method in the Tabular class is just a wrapper for this class' Query method... So use that instead. Args AdomdConnection ( type ) : description Methods: .query source .query( query_str: str ) Executes Query on Model and Returns results in Pandas DataFrame Args query_str (str) : Dax Query. Note, needs full syntax (ex: EVALUATE). See (DAX Queries)[https://docs.microsoft.com/en-us/dax/dax-queries]. Will check if query string is a file. If it is, then it will perform a query on whatever is read from the file. It is also possible to query DMV. For example. Query(\"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\"). See (DMVs)[https://docs.microsoft.com/en-us/analysis-services/instances/use-dynamic-management-views-dmvs-to-monitor-analysis-services?view=asallproducts-allversions] Returns DataFrame : Returns dataframe with results","title":"Queries"},{"location":"Queries/#_1","text":"","title":""},{"location":"Queries/#connection","text":"source Connection( server, effective_user = None ) Subclass for Adomdclient . With some extra items on top. Right now designed for internal use. For example, Query method in the Tabular class is just a wrapper for this class' Query method... So use that instead. Args AdomdConnection ( type ) : description Methods:","title":"Connection"},{"location":"Queries/#query","text":"source .query( query_str: str ) Executes Query on Model and Returns results in Pandas DataFrame Args query_str (str) : Dax Query. Note, needs full syntax (ex: EVALUATE). See (DAX Queries)[https://docs.microsoft.com/en-us/dax/dax-queries]. Will check if query string is a file. If it is, then it will perform a query on whatever is read from the file. It is also possible to query DMV. For example. Query(\"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\"). See (DMVs)[https://docs.microsoft.com/en-us/analysis-services/instances/use-dynamic-management-views-dmvs-to-monitor-analysis-services?view=asallproducts-allversions] Returns DataFrame : Returns dataframe with results","title":".query"},{"location":"Refreshes/","text":"PyRefresh source PyRefresh( model, object: Union[str, PyTable, PyPartition, Dict[str, Any]], trace: BaseTrace = RefreshTrace, refresh_checks: RefreshCheckCollection = RefreshCheckCollection(), default_row_count_check: bool = True, refresh_type: RefreshType = RefreshType.Full ) PyRefresh Class to handle refreshes of model. Args model (Tabular) : Main Tabular Class object (Union[str, PyTable, PyPartition, Dict[str, Any]]) : Designed to handle a few different ways of selecting a refresh. Can be a string of 'Table Name' or dict of {'Table Name': 'Partition Name'} or even some combination with the actual PyTable and PyPartition classes. trace (BaseTrace, optional) : Set to None if no Tracing is desired, otherwise you can use default trace or create your own. Defaults to RefreshTrace. RefreshChecks (RefreshCheckCollection, optional) : Add your RefreshCheck 's into a RefreshCheckCollection . Defaults to RefreshCheckCollection(). default_row_count_check (bool, optional) : Quick built in check will fail the refresh if post check row count is zero. Defaults to True. refresh_type (RefreshType, optional) : Input RefreshType desired. Defaults to RefreshType.Full. Methods: .run source .run() Brings it all together. When ready, executes all the pre checks. Then refreshes. Then runs all the post checks. RefreshCheck source RefreshCheck( name: str, function, assertion = None ) RefreshCheck is an assertion you run after your refreshes. It will run the given function before and after refreshes, then run the assertion of before and after. The default given in a refresh is to check row count. It will check row count before, and row count after. Then fail if row count after is zero. Args name (str) : Name of RefreshCheck function (function) : Function to run before and after refresh. assertion (function) : Functino that takes a pre and post argument to output True or False. Methods: .name source .name() Get your custom name of refresh check. .function source .function() Get the function that is used to run a pre and post check. .pre source .pre() Get the pre value that is the result from the pre refresh check. .post source .post() Get the post value that is the result from the post refresh check. .assertion source .assertion() Get the assertion that is the result from the post refresh check. .pre_check source .pre_check() Runs self._check(\"Pre\") .post_check source .post_check() Runs self._check(\"Post\") then self.assertion_run() .assertion_run source .assertion_run() Runs the given self.assertion function with self.pre and self.post . So, self.assertion_run(self.pre, self.post) . RefreshCheckCollection source RefreshCheckCollection( refresh_checks: RefreshCheck = [] ) Groups together your RefreshChecks to handle multiple types of checks in a single refresh. Methods: .add_refresh_check source .add_refresh_check( refresh_check: RefreshCheck ) Add a RefreshCheck Args RefreshCheck (RefreshCheck) : RefreshCheck class. .remove_refresh_check source .remove_refresh_check( refresh_check: RefreshCheck ) Remove a RefreshCheck Args RefreshCheck (RefreshCheck) : RefreshCheck class. .clear_refresh_checks source .clear_refresh_checks() Clear Refresh Checks.","title":"Refreshes"},{"location":"Refreshes/#_1","text":"","title":""},{"location":"Refreshes/#pyrefresh","text":"source PyRefresh( model, object: Union[str, PyTable, PyPartition, Dict[str, Any]], trace: BaseTrace = RefreshTrace, refresh_checks: RefreshCheckCollection = RefreshCheckCollection(), default_row_count_check: bool = True, refresh_type: RefreshType = RefreshType.Full ) PyRefresh Class to handle refreshes of model. Args model (Tabular) : Main Tabular Class object (Union[str, PyTable, PyPartition, Dict[str, Any]]) : Designed to handle a few different ways of selecting a refresh. Can be a string of 'Table Name' or dict of {'Table Name': 'Partition Name'} or even some combination with the actual PyTable and PyPartition classes. trace (BaseTrace, optional) : Set to None if no Tracing is desired, otherwise you can use default trace or create your own. Defaults to RefreshTrace. RefreshChecks (RefreshCheckCollection, optional) : Add your RefreshCheck 's into a RefreshCheckCollection . Defaults to RefreshCheckCollection(). default_row_count_check (bool, optional) : Quick built in check will fail the refresh if post check row count is zero. Defaults to True. refresh_type (RefreshType, optional) : Input RefreshType desired. Defaults to RefreshType.Full. Methods:","title":"PyRefresh"},{"location":"Refreshes/#run","text":"source .run() Brings it all together. When ready, executes all the pre checks. Then refreshes. Then runs all the post checks.","title":".run"},{"location":"Refreshes/#refreshcheck","text":"source RefreshCheck( name: str, function, assertion = None ) RefreshCheck is an assertion you run after your refreshes. It will run the given function before and after refreshes, then run the assertion of before and after. The default given in a refresh is to check row count. It will check row count before, and row count after. Then fail if row count after is zero. Args name (str) : Name of RefreshCheck function (function) : Function to run before and after refresh. assertion (function) : Functino that takes a pre and post argument to output True or False. Methods:","title":"RefreshCheck"},{"location":"Refreshes/#name","text":"source .name() Get your custom name of refresh check.","title":".name"},{"location":"Refreshes/#function","text":"source .function() Get the function that is used to run a pre and post check.","title":".function"},{"location":"Refreshes/#pre","text":"source .pre() Get the pre value that is the result from the pre refresh check.","title":".pre"},{"location":"Refreshes/#post","text":"source .post() Get the post value that is the result from the post refresh check.","title":".post"},{"location":"Refreshes/#assertion","text":"source .assertion() Get the assertion that is the result from the post refresh check.","title":".assertion"},{"location":"Refreshes/#pre_check","text":"source .pre_check() Runs self._check(\"Pre\")","title":".pre_check"},{"location":"Refreshes/#post_check","text":"source .post_check() Runs self._check(\"Post\") then self.assertion_run()","title":".post_check"},{"location":"Refreshes/#assertion_run","text":"source .assertion_run() Runs the given self.assertion function with self.pre and self.post . So, self.assertion_run(self.pre, self.post) .","title":".assertion_run"},{"location":"Refreshes/#refreshcheckcollection","text":"source RefreshCheckCollection( refresh_checks: RefreshCheck = [] ) Groups together your RefreshChecks to handle multiple types of checks in a single refresh. Methods:","title":"RefreshCheckCollection"},{"location":"Refreshes/#add_refresh_check","text":"source .add_refresh_check( refresh_check: RefreshCheck ) Add a RefreshCheck Args RefreshCheck (RefreshCheck) : RefreshCheck class.","title":".add_refresh_check"},{"location":"Refreshes/#remove_refresh_check","text":"source .remove_refresh_check( refresh_check: RefreshCheck ) Remove a RefreshCheck Args RefreshCheck (RefreshCheck) : RefreshCheck class.","title":".remove_refresh_check"},{"location":"Refreshes/#clear_refresh_checks","text":"source .clear_refresh_checks() Clear Refresh Checks.","title":".clear_refresh_checks"},{"location":"Table/","text":"PyTable source PyTable( object, model ) Wrapper for Table Class . With a few other bells and whistles added to it. Notice the PyObject magic method __getattr__() will search in self._object if it is unable to find it in the default attributes. This let's you also easily check the default .Net properties. Methods: .row_count source .row_count() Method to return count of rows. Simple Dax Query: EVALUATE {COUNTROWS('Table Name')} Returns int : Number of rows using COUNTROWS . .refresh source .refresh( *args, **kwargs ) Same method from Model Refresh, you can pass through any extra parameters. For example: Tabular().Tables['Table Name'].Refresh(Tracing = True) Returns DataFrame : Returns pandas dataframe with some refresh details .last_refresh source .last_refresh() Will query each partition for the last refresh time then select the max Returns datetime : Last refresh time in datetime format .related source .related() Returns tables with a relationship with the table in question. PyTables source PyTables( objects ) Groups together multiple tables. See PyObjects class for what more it can do. You can interact with PyTables straight from model. For ex: model.Tables . You can even filter down with .Find() . For example find all tables with fact in name. model.Tables.Find('fact') . Methods: .refresh source .refresh( *args, **kwargs ) Refreshes all PyTable (s) in class. .query_all source .query_all( query_function: str = 'COUNTROWS(_)' ) This will dynamically create a query to pull all tables from the model and run the query function. It will replace the _ with the table to run. Args query_function (str, optional) : Dax query is dynamically building a query with the UNION & ROW DAX Functions. Defaults to 'COUNTROWS(_)'. Returns DataFrame : Returns dataframe with results .find_zero_rows source .find_zero_rows() Returns PyTables class of tables with zero rows queried. .last_refresh source .last_refresh( group_partition: bool = True ) Returns pd.DataFrame of tables with their latest refresh time. Optional 'group_partition' variable, default is True. If False an extra column will be include to have the last refresh time to the grain of the partition Example to add to model model.Create_Table(p.Table_Last_Refresh_Times(model),'RefreshTimes') Args model (pytabular.Tabular) : Tabular Model group_partition (bool, optional) : Whether or not you want the grain of the dataframe to be by table or by partition. Defaults to True. Returns DataFrame : pd dataframe with the RefreshedTime property: https://docs.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.partition.refreshedtime?view=analysisservices-dotnet#microsoft-analysisservices-tabular-partition-refreshedtime If group_partition == True and the table has multiple partitions, then df.groupby(by[\"tables\"]).max()","title":"Table"},{"location":"Table/#_1","text":"","title":""},{"location":"Table/#pytable","text":"source PyTable( object, model ) Wrapper for Table Class . With a few other bells and whistles added to it. Notice the PyObject magic method __getattr__() will search in self._object if it is unable to find it in the default attributes. This let's you also easily check the default .Net properties. Methods:","title":"PyTable"},{"location":"Table/#row_count","text":"source .row_count() Method to return count of rows. Simple Dax Query: EVALUATE {COUNTROWS('Table Name')} Returns int : Number of rows using COUNTROWS .","title":".row_count"},{"location":"Table/#refresh","text":"source .refresh( *args, **kwargs ) Same method from Model Refresh, you can pass through any extra parameters. For example: Tabular().Tables['Table Name'].Refresh(Tracing = True) Returns DataFrame : Returns pandas dataframe with some refresh details","title":".refresh"},{"location":"Table/#last_refresh","text":"source .last_refresh() Will query each partition for the last refresh time then select the max Returns datetime : Last refresh time in datetime format","title":".last_refresh"},{"location":"Table/#related","text":"source .related() Returns tables with a relationship with the table in question.","title":".related"},{"location":"Table/#pytables","text":"source PyTables( objects ) Groups together multiple tables. See PyObjects class for what more it can do. You can interact with PyTables straight from model. For ex: model.Tables . You can even filter down with .Find() . For example find all tables with fact in name. model.Tables.Find('fact') . Methods:","title":"PyTables"},{"location":"Table/#refresh_1","text":"source .refresh( *args, **kwargs ) Refreshes all PyTable (s) in class.","title":".refresh"},{"location":"Table/#query_all","text":"source .query_all( query_function: str = 'COUNTROWS(_)' ) This will dynamically create a query to pull all tables from the model and run the query function. It will replace the _ with the table to run. Args query_function (str, optional) : Dax query is dynamically building a query with the UNION & ROW DAX Functions. Defaults to 'COUNTROWS(_)'. Returns DataFrame : Returns dataframe with results","title":".query_all"},{"location":"Table/#find_zero_rows","text":"source .find_zero_rows() Returns PyTables class of tables with zero rows queried.","title":".find_zero_rows"},{"location":"Table/#last_refresh_1","text":"source .last_refresh( group_partition: bool = True ) Returns pd.DataFrame of tables with their latest refresh time. Optional 'group_partition' variable, default is True. If False an extra column will be include to have the last refresh time to the grain of the partition Example to add to model model.Create_Table(p.Table_Last_Refresh_Times(model),'RefreshTimes') Args model (pytabular.Tabular) : Tabular Model group_partition (bool, optional) : Whether or not you want the grain of the dataframe to be by table or by partition. Defaults to True. Returns DataFrame : pd dataframe with the RefreshedTime property: https://docs.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.partition.refreshedtime?view=analysisservices-dotnet#microsoft-analysisservices-tabular-partition-refreshedtime If group_partition == True and the table has multiple partitions, then df.groupby(by[\"tables\"]).max()","title":".last_refresh"},{"location":"Tabular%20Editor%202/","text":"TabularEditor source TabularEditor( exe_file_path: str = 'Default' ) Setting Tabular_Editor Class for future work. Mainly runs download_tabular_editor() download_tabular_editor source .download_tabular_editor( download_location: str = 'https: //github.com/TabularEditor/TabularEditor/releases/download/2.16.7/TabularEditor.Portable.zip', folder: str = 'Tabular_Editor_2', auto_remove = True ) Runs a request.get() to retrieve the zip file from web. Will unzip response and store in directory. Will also register the removal of the new directory and files when exiting program. Args download_location (str, optional) : File path for zip of Tabular Editor 2. Defaults to [Tabular Editor 2 Github Zip Location]'https://github.com/TabularEditor/TabularEditor/releases/download/2.16.7/TabularEditor.Portable.zip'. folder (str, optional) : New Folder Location. Defaults to 'Tabular_Editor_2'. auto_remove (bool, optional) : Boolean to determine auto removal of files once script exits. Defaults to True. Returns str : File path of TabularEditor.exe","title":"Tabular Editor"},{"location":"Tabular%20Editor%202/#_1","text":"","title":""},{"location":"Tabular%20Editor%202/#tabulareditor","text":"source TabularEditor( exe_file_path: str = 'Default' ) Setting Tabular_Editor Class for future work. Mainly runs download_tabular_editor()","title":"TabularEditor"},{"location":"Tabular%20Editor%202/#download_tabular_editor","text":"source .download_tabular_editor( download_location: str = 'https: //github.com/TabularEditor/TabularEditor/releases/download/2.16.7/TabularEditor.Portable.zip', folder: str = 'Tabular_Editor_2', auto_remove = True ) Runs a request.get() to retrieve the zip file from web. Will unzip response and store in directory. Will also register the removal of the new directory and files when exiting program. Args download_location (str, optional) : File path for zip of Tabular Editor 2. Defaults to [Tabular Editor 2 Github Zip Location]'https://github.com/TabularEditor/TabularEditor/releases/download/2.16.7/TabularEditor.Portable.zip'. folder (str, optional) : New Folder Location. Defaults to 'Tabular_Editor_2'. auto_remove (bool, optional) : Boolean to determine auto removal of files once script exits. Defaults to True. Returns str : File path of TabularEditor.exe","title":"download_tabular_editor"},{"location":"Tabular/","text":"Tabular source Tabular( connection_str: str ) Tabular Class to perform operations: Microsoft.AnalysisServices.Tabular . You can use this class as your main way to interact with your model. Args connection_str (str) : Valid Connection String for connecting to a Tabular Model. Attributes Server (Server) : See Server MS Docs . Catalog (str) : Name of Database. See Catalog MS Docs . Model (Model) : See Model MS Docs . AdomdConnection (AdomdConnection) : For querying. See AdomdConnection MS Docs . Connection made from parts of the originally provided connection string. Tables (PyTables[PyTable]) : Wrappers for Table MS Docs . So you have the full capabilities of what the MS Docs offer and a few others. Like Tabular().Tables['Table Name'].Row_Count() . Or you can find a table via Tabular().Tables[0] or Tabular().Tables['Table Name'] Columns (List[Column]) : Easy access list of columns from model. See Column MS Docs . Partitions (List[Partition]) : Easy access list of partitions from model. See Partition MS Docs . Measures (List[Measure]) : Easy access list of measures from model. See Measure MS Docs . Methods: .reload_model_info source .reload_model_info() Runs on init iterates through details, can be called after any model changes. Called in save_changes() Returns bool : True if successful .is_process source .is_process() Run method to check if Processing is occurring. Will query DMV $SYSTEM.DISCOVER_JOBS to see if any processing is happening. Returns bool : True if DMV shows Process, False if not. .disconnect source .disconnect() Disconnects from Model .reconnect source .reconnect() Reconnects to Model .refresh source .refresh( *args, **kwargs ) PyRefresh Class to handle refreshes of model. Args model (Tabular) : Main Tabular Class object (Union[str, PyTable, PyPartition, Dict[str, Any]]) : Designed to handle a few different ways of selecting a refresh. Can be a string of 'Table Name' or dict of {'Table Name': 'Partition Name'} or even some combination with the actual PyTable and PyPartition classes. trace (Base_Trace, optional) : Set to None if no Tracing is desired, otherwise you can use default trace or create your own. Defaults to Refresh_Trace. refresh_checks (Refresh_Check_Collection, optional) : Add your Refresh_Check 's into a Refresh_Check_Collection . Defaults to Refresh_Check_Collection(). default_row_count_check (bool, optional) : Quick built in check will fail the refresh if post check row count is zero. Defaults to True. refresh_type (RefreshType, optional) : Input RefreshType desired. Defaults to RefreshType.Full. Returns pd.DataFrame .save_changes source .save_changes() Called after refreshes or any model changes. Currently will return a named tuple of all changes detected. However a ton of room for improvement here. .backup_table source .backup_table( table_str: str ) Will be removed. This is experimental with no written pytest for it. Backs up table in memory, brings with it measures, columns, hierarchies, relationships, roles, etc. It will add suffix '_backup' to all objects. Refresh is performed from source during backup. Args table_str (str, optional) : Name of Table. Returns bool : Returns True if Successful, else will return error. .revert_table source .revert_table( table_str: str ) Will be removed. This is experimental with no written pytest for it. This is used in conjunction with Backup_Table(). It will take the 'TableName_backup' and replace with the original. Example scenario -> 1. model.Backup_Table('TableName') 2. perform any proposed changes in original 'TableName' 3. validate changes in 'TableName' 4. if unsuccessful run model.Revert_Table('TableName') Args table_str (str) : Name of table. Returns bool : Returns True if Successful, else will return error. .query source .query( query_str: str, effective_user: str = None ) Executes Query on Model and Returns Results in Pandas DataFrame Args query_Str (str) : Dax Query. Note, needs full syntax (ex: EVALUATE). See (DAX Queries)[https://docs.microsoft.com/en-us/dax/dax-queries]. effective_user (str) : User you wish to query as. Will check if query string is a file. If it is, then it will perform a query on whatever is read from the file. It is also possible to query DMV. For example. Query(\"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\"). See (DMVs)[https://docs.microsoft.com/en-us/analysis-services/instances/use-dynamic-management-views-dmvs-to-monitor-analysis-services?view=asallproducts-allversions] Returns DataFrame : Returns dataframe with results .analyze_bpa source .analyze_bpa( tabular_editor_exe: str, best_practice_analyzer: str ) Takes your Tabular Model and performs TE2s BPA. Runs through Command line. Tabular Editor BPA Tabular Editor Command Line Options Args tabular_editor_exe (str) : TE2 Exe File path. Feel free to use class TE2().EXE_Path or provide your own. best_practice_analyzer (str) : BPA json file path. Feel free to use class BPA().Location or provide your own. Defualts to https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json Returns Assuming no failure, will return list of BPA violations. Else will return error from command line. .create_table source .create_table( df: pd.DataFrame, table_name: str ) Creates tables from pd.DataFrame as an M-Partition. So will convert the dataframe to M-Partition logic via the M query table constructor. Runs refresh and will update model. Args df (pd.DataFrame) : DataFrame to add to model table_name (str) : description Returns bool : True if successful","title":"Tabular"},{"location":"Tabular/#_1","text":"","title":""},{"location":"Tabular/#tabular","text":"source Tabular( connection_str: str ) Tabular Class to perform operations: Microsoft.AnalysisServices.Tabular . You can use this class as your main way to interact with your model. Args connection_str (str) : Valid Connection String for connecting to a Tabular Model. Attributes Server (Server) : See Server MS Docs . Catalog (str) : Name of Database. See Catalog MS Docs . Model (Model) : See Model MS Docs . AdomdConnection (AdomdConnection) : For querying. See AdomdConnection MS Docs . Connection made from parts of the originally provided connection string. Tables (PyTables[PyTable]) : Wrappers for Table MS Docs . So you have the full capabilities of what the MS Docs offer and a few others. Like Tabular().Tables['Table Name'].Row_Count() . Or you can find a table via Tabular().Tables[0] or Tabular().Tables['Table Name'] Columns (List[Column]) : Easy access list of columns from model. See Column MS Docs . Partitions (List[Partition]) : Easy access list of partitions from model. See Partition MS Docs . Measures (List[Measure]) : Easy access list of measures from model. See Measure MS Docs . Methods:","title":"Tabular"},{"location":"Tabular/#reload_model_info","text":"source .reload_model_info() Runs on init iterates through details, can be called after any model changes. Called in save_changes() Returns bool : True if successful","title":".reload_model_info"},{"location":"Tabular/#is_process","text":"source .is_process() Run method to check if Processing is occurring. Will query DMV $SYSTEM.DISCOVER_JOBS to see if any processing is happening. Returns bool : True if DMV shows Process, False if not.","title":".is_process"},{"location":"Tabular/#disconnect","text":"source .disconnect() Disconnects from Model","title":".disconnect"},{"location":"Tabular/#reconnect","text":"source .reconnect() Reconnects to Model","title":".reconnect"},{"location":"Tabular/#refresh","text":"source .refresh( *args, **kwargs ) PyRefresh Class to handle refreshes of model. Args model (Tabular) : Main Tabular Class object (Union[str, PyTable, PyPartition, Dict[str, Any]]) : Designed to handle a few different ways of selecting a refresh. Can be a string of 'Table Name' or dict of {'Table Name': 'Partition Name'} or even some combination with the actual PyTable and PyPartition classes. trace (Base_Trace, optional) : Set to None if no Tracing is desired, otherwise you can use default trace or create your own. Defaults to Refresh_Trace. refresh_checks (Refresh_Check_Collection, optional) : Add your Refresh_Check 's into a Refresh_Check_Collection . Defaults to Refresh_Check_Collection(). default_row_count_check (bool, optional) : Quick built in check will fail the refresh if post check row count is zero. Defaults to True. refresh_type (RefreshType, optional) : Input RefreshType desired. Defaults to RefreshType.Full. Returns pd.DataFrame","title":".refresh"},{"location":"Tabular/#save_changes","text":"source .save_changes() Called after refreshes or any model changes. Currently will return a named tuple of all changes detected. However a ton of room for improvement here.","title":".save_changes"},{"location":"Tabular/#backup_table","text":"source .backup_table( table_str: str ) Will be removed. This is experimental with no written pytest for it. Backs up table in memory, brings with it measures, columns, hierarchies, relationships, roles, etc. It will add suffix '_backup' to all objects. Refresh is performed from source during backup. Args table_str (str, optional) : Name of Table. Returns bool : Returns True if Successful, else will return error.","title":".backup_table"},{"location":"Tabular/#revert_table","text":"source .revert_table( table_str: str ) Will be removed. This is experimental with no written pytest for it. This is used in conjunction with Backup_Table(). It will take the 'TableName_backup' and replace with the original. Example scenario -> 1. model.Backup_Table('TableName') 2. perform any proposed changes in original 'TableName' 3. validate changes in 'TableName' 4. if unsuccessful run model.Revert_Table('TableName') Args table_str (str) : Name of table. Returns bool : Returns True if Successful, else will return error.","title":".revert_table"},{"location":"Tabular/#query","text":"source .query( query_str: str, effective_user: str = None ) Executes Query on Model and Returns Results in Pandas DataFrame Args query_Str (str) : Dax Query. Note, needs full syntax (ex: EVALUATE). See (DAX Queries)[https://docs.microsoft.com/en-us/dax/dax-queries]. effective_user (str) : User you wish to query as. Will check if query string is a file. If it is, then it will perform a query on whatever is read from the file. It is also possible to query DMV. For example. Query(\"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\"). See (DMVs)[https://docs.microsoft.com/en-us/analysis-services/instances/use-dynamic-management-views-dmvs-to-monitor-analysis-services?view=asallproducts-allversions] Returns DataFrame : Returns dataframe with results","title":".query"},{"location":"Tabular/#analyze_bpa","text":"source .analyze_bpa( tabular_editor_exe: str, best_practice_analyzer: str ) Takes your Tabular Model and performs TE2s BPA. Runs through Command line. Tabular Editor BPA Tabular Editor Command Line Options Args tabular_editor_exe (str) : TE2 Exe File path. Feel free to use class TE2().EXE_Path or provide your own. best_practice_analyzer (str) : BPA json file path. Feel free to use class BPA().Location or provide your own. Defualts to https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json Returns Assuming no failure, will return list of BPA violations. Else will return error from command line.","title":".analyze_bpa"},{"location":"Tabular/#create_table","text":"source .create_table( df: pd.DataFrame, table_name: str ) Creates tables from pd.DataFrame as an M-Partition. So will convert the dataframe to M-Partition logic via the M query table constructor. Runs refresh and will update model. Args df (pd.DataFrame) : DataFrame to add to model table_name (str) : description Returns bool : True if successful","title":".create_table"},{"location":"Traces/","text":"BaseTrace source BaseTrace( tabular_class, trace_events: List[TraceEvent], trace_event_columns: List[TraceColumn], handler: Callable ) Generates Trace to be run on Server. This is the base class to customize the type of Trace you are looking for. Server Traces Args tabular_class (Tabular) : Tabular Class. trace_events (List[TraceEvent]) : List of Trace Events. trace_event_columns (List[TraceColumn]) : List of Trace Event Columns. Handler (Callable) : Function to call when Trace returns response. TraceEventClass TraceEventColumn Input needs to be two arguments. One is source (Which is currently None... Need to investigate why). Second is TraceEventArgs Methods: .build source .build() Run on initialization. This will take the inputed arguments for the class and attempt to build the Trace. Returns bool : True if successful .add source .add() Runs on initialization. Adds built Trace to the Server. Returns int : Return int of placement in Server.Traces.get_Item(int) .update source .update() Runs on initialization. Syncs with Server. Returns None : Returns None. Unless unsuccessful then it will return the error from Server. .start source .start() Call when you want to start the Trace Returns None : Returns None. Unless unsuccessful then it will return the error from Server. .stop source .stop() Call when you want to stop the Trace Returns None : Returns None. Unless unsuccessful then it will return the error from Server. .drop source .drop() Call when you want to drop the Trace Returns None : Returns None. Unless unsuccessful, then it will return the error from Server. RefreshTrace source RefreshTrace( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.ProgressReportBegin, TraceEventClass.ProgressReportCurrent, TraceEventClass.ProgressReportEnd, TraceEventClass.ProgressReportError], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.CurrentTime, TraceColumn.ObjectName, TraceColumn.ObjectPath, TraceColumn.DatabaseName, TraceColumn.SessionID, TraceColumn.TextData, TraceColumn.EventClass, TraceColumn.ProgressTotal], handler: Callable = _refresh_handler ) Subclass of BaseTrace. For built-in Refresh Tracing. Run by default when refreshing tables or partitions. Args BaseTrace (BaseTrace) : BaseTrace Class QueryMonitor source QueryMonitor( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.QueryEnd], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.StartTime, TraceColumn.EndTime, TraceColumn.Duration, TraceColumn.Severity, TraceColumn.Error, TraceColumn.NTUserName, TraceColumn.DatabaseName, TraceColumn.ApplicationName, TraceColumn.TextData], handler: Callable = _query_monitor_handler ) Subclass of BaseTrace. For built-in Query Monitoring. If you want to see full query text, set logger to debug. Args BaseTrace (BaseTrace) : BaseTrace Class","title":"Trace"},{"location":"Traces/#_1","text":"","title":""},{"location":"Traces/#basetrace","text":"source BaseTrace( tabular_class, trace_events: List[TraceEvent], trace_event_columns: List[TraceColumn], handler: Callable ) Generates Trace to be run on Server. This is the base class to customize the type of Trace you are looking for. Server Traces Args tabular_class (Tabular) : Tabular Class. trace_events (List[TraceEvent]) : List of Trace Events. trace_event_columns (List[TraceColumn]) : List of Trace Event Columns. Handler (Callable) : Function to call when Trace returns response. TraceEventClass TraceEventColumn Input needs to be two arguments. One is source (Which is currently None... Need to investigate why). Second is TraceEventArgs Methods:","title":"BaseTrace"},{"location":"Traces/#build","text":"source .build() Run on initialization. This will take the inputed arguments for the class and attempt to build the Trace. Returns bool : True if successful","title":".build"},{"location":"Traces/#add","text":"source .add() Runs on initialization. Adds built Trace to the Server. Returns int : Return int of placement in Server.Traces.get_Item(int)","title":".add"},{"location":"Traces/#update","text":"source .update() Runs on initialization. Syncs with Server. Returns None : Returns None. Unless unsuccessful then it will return the error from Server.","title":".update"},{"location":"Traces/#start","text":"source .start() Call when you want to start the Trace Returns None : Returns None. Unless unsuccessful then it will return the error from Server.","title":".start"},{"location":"Traces/#stop","text":"source .stop() Call when you want to stop the Trace Returns None : Returns None. Unless unsuccessful then it will return the error from Server.","title":".stop"},{"location":"Traces/#drop","text":"source .drop() Call when you want to drop the Trace Returns None : Returns None. Unless unsuccessful, then it will return the error from Server.","title":".drop"},{"location":"Traces/#refreshtrace","text":"source RefreshTrace( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.ProgressReportBegin, TraceEventClass.ProgressReportCurrent, TraceEventClass.ProgressReportEnd, TraceEventClass.ProgressReportError], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.CurrentTime, TraceColumn.ObjectName, TraceColumn.ObjectPath, TraceColumn.DatabaseName, TraceColumn.SessionID, TraceColumn.TextData, TraceColumn.EventClass, TraceColumn.ProgressTotal], handler: Callable = _refresh_handler ) Subclass of BaseTrace. For built-in Refresh Tracing. Run by default when refreshing tables or partitions. Args BaseTrace (BaseTrace) : BaseTrace Class","title":"RefreshTrace"},{"location":"Traces/#querymonitor","text":"source QueryMonitor( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.QueryEnd], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.StartTime, TraceColumn.EndTime, TraceColumn.Duration, TraceColumn.Severity, TraceColumn.Error, TraceColumn.NTUserName, TraceColumn.DatabaseName, TraceColumn.ApplicationName, TraceColumn.TextData], handler: Callable = _query_monitor_handler ) Subclass of BaseTrace. For built-in Query Monitoring. If you want to see full query text, set logger to debug. Args BaseTrace (BaseTrace) : BaseTrace Class","title":"QueryMonitor"},{"location":"contributing/","text":"Contributing Guidelines Work will be distributed under MIT license. See github actions for checks run on pull request. Updates of any kind are welcome! Even just letting me know of the issues. Or updating doc strings... Limit any external modules, see pyproject.toml for dependencies Goal of project is to help connect the python world and the tabular model world for some easier programmatic execution on models. Install pre-commit. Pre-commit will run pytest, flake8, and docstr-coverage before push. pip install pre-commit pre-commit install --hook-type pre-push This will take a while... pytest will open a PBIX file in repository and run tests on it.","title":"Contributing Guidelines"},{"location":"contributing/#contributing-guidelines","text":"Work will be distributed under MIT license. See github actions for checks run on pull request. Updates of any kind are welcome! Even just letting me know of the issues. Or updating doc strings... Limit any external modules, see pyproject.toml for dependencies Goal of project is to help connect the python world and the tabular model world for some easier programmatic execution on models. Install pre-commit. Pre-commit will run pytest, flake8, and docstr-coverage before push. pip install pre-commit pre-commit install --hook-type pre-push This will take a while... pytest will open a PBIX file in repository and run tests on it.","title":"Contributing Guidelines"}]}