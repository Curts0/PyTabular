{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTabular What is it? PyTabular (python-tabular in pypi ) is a python package that allows for programmatic execution on your tabular models! This is possible thanks to Pythonnet and Microsoft's .Net APIs on Azure Analysis Services . Currently, this build is tested and working on Windows Operating System only. Help is needed to expand this for other operating systems. The package should have the dll files included when you import it. See Documentation Here . PyTabular is still considered alpha while I'm working on building out the proper tests and testing environments, so I can ensure some kind of stability in features. Please send bugs my way! Preferably in the issues section in Github. I want to harden this project so many can use it easily. I currently have local pytest for python 3.6 to 3.10 and run those tests through a local AAS and Gen2 model. Getting Started See the Pypi project for available versions. To become PEP8 compliant with naming conventions, serious name changes were made in 0.3.5. Install v. 0.3.4 or lower to get the older naming conventions. python3 -m pip install python-tabular #install specific version python3 -m pip install python-tabular==0.3.4 In your python environment, import pytabular and call the main Tabular Class. Only parameter needed is a solid connection string. import pytabular model = pytabular.Tabular(CONNECTION_STR) I'm a big fan of logging, if you don't want any just get the logger and disable it. import pytabular pytabular.logger.disabled = True You can query your models with the Query method from your tabular class. For Dax Queries, it will need the full Dax syntax. See EVALUATE example . This will return a Pandas DataFrame . If you are looking to return a single value, see below. Simply wrap your query in the the curly brackets. The method will take that single cell table and just return the individual value. You can also query your DMV. See below for example. See PyTabular Docs for Query . #Run basic queries DAX_QUERY = \"EVALUATE TOPN(100, 'Table1')\" model.query(DAX_QUERY) #returns pd.DataFrame() #or... DMV_QUERY = \"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\" model.query(DMV_QUERY) #returns pd.DataFrame() #or... SINGLE_VALUE_QUERY_EX = \"EVALUATE {1}\" model.query(SINGLE_VALUE_QUERY_EX) #returns 1 #or... FILE_PATH = 'C:\\\\FILEPATHEXAMPLE\\\\file.dax' #or file.txt model.query(FILE_PATH) #Will return same logic as above, single values if possible else will return pd.DataFrame() You can also explore your tables, partitions, and columns. Via the Attributes from your Tabular class. #Explore tables... dir(model.Tables['Table Name']) #Explore columns & partitions dir(model.Tables['Table Name'].Partitions['Partition Name']) #Only a few features right now, but check out the built in methods. model.Tables['Table Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].last_refresh() #or model.Tables['Table Name'].row_count() #or model.Tables['Table Name'].Columns['Column Name'].distinct_count() Refresh method to handle refreshes on your model. This is synchronous. Should be flexible enough to handle a variety of inputs. See PyTabular Docs for Refreshing Tables and Partitions . Most basic way to refresh is input the table name string. The method will search for table and output exeption if unable to find it. For partitions you will need a key, value combination. Example, {'Table1':'Partition1'} . You can also take the key value pair and iterate through a group of partitions. Example, {'Table1':['Partition1','Partition2']} . Rather than providing a string, you can also input the actual class. See below for those examples, and you can acess them from the built in attributes self.Tables , self.Partitions or explore through the .Net classes yourself in self.Model.Tables . #You have a few options when refreshing. model.refresh('Table Name') #or... model.refresh(['Table1','Table2','Table3']) #or... model.refresh(<Table Class>) #or... model.refresh(<Partition Class>) #or... model.refresh({'Table Name':'Partition Name'}) #or any kind of weird combination like model.refresh([{<Table Class>:<Partition Class>,'Table Name':['Partition1','Partition2']},'Table Name','Table Name2']) #You can even run through the Tables & Partition Attributes model.Tables['Table Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].refresh() #Default Tracing happens automatically, but can be removed by... model.refresh(['Table1','Table2'], trace = None) It's not uncommon to need to run through some checks on specific Tables, Partitions, Columns, Etc... #Get Row Count from model model.Tables['Table Name'].row_count() #Get Last Refresh time from a partition model.Tables['Table Name'].last_refresh() #Get Distinct Count or Values from a Column model.Tables['Table Name'].Columns['Column Name'].distinct_count() #or model.Tables['Table Name'].Columns['Column Name'].values() Use Cases If blank table, then refresh table. This will use the function Return_Zero_Row_Tables and the method Refresh from the Tabular class. import pytabular model = pytabular.Tabular(CONNECTION_STR) tables = model.Tables.find_zero_rows() if len(tables) > 0: model.refresh(tables) Sneak in a refresh. This will use the method Is_Process and the method Refresh from the Tabular class. It will check the DMV to see if any jobs are currently running classified as processing. import pytabular model = pytabular.Tabular(CONNECTION_STR) if model.is_process(): #do what you want if there is a refresh happening else: model.refresh(TABLES_OR_PARTITIONS_TO_REFRESH) Show refresh times in model. This will use the function Table_Last_Refresh_Times and the method Create_Table from the Tabular class. It will search through the model for all tables and partitions and pull the 'RefreshedTime' property from it. It will return results into a pandas dataframe, which will then be converted into an M expression used for a new table. import pytabular model = pytabular.Tabular(CONNECTION_STR) df = model.Tables.last_refresh() model.create_table(df, 'Refresh Times') If BPA Violation, then revert deployment. Uses a few things. First the BPA Class , then the TE2 Class , and will finish with the Analyze_BPA method. Did not want to re-invent the wheel with the amazing work done with Tabular Editor and it's BPA capabilities. import pytabular model = pytabular.Tabular(CONNECTION_STR) te2 = pytabular.TabularEditor() #Feel free to input your TE2 File path or this will download for you. bpa = pytabular.BPA() #Fee free to input your own BPA file or this will download for you from: https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json results = model.analyze_bpa(te2.exe,bpa.location) if len(results) > 0: #Revert deployment here! Loop through and query Dax files Let's say you have multiple dax queries you would like to store and run through as checks. The Query method on the Tabular class can also take file paths. Can really be any file type as it's just checking os.path.isfile(). But would suggest .dax or .txt. It will read the file that use that as the new Query_str argument. import pytabular model = pytabular.Tabular(CONNECTION_STR) LIST_OF_FILE_PATHS = ['C:\\\\FilePath\\\\file1.dax','C:\\\\FilePath\\\\file1.txt','C:\\\\FilePath\\\\file2.dax','C:\\\\FilePath\\\\file2.txt'] for file_path in LIST_OF_FILE_PATHS: model.query(file_path) Advanced Refreshing with Pre and Post Checks Maybe you are introducing new logic to a fact table, and you need to ensure that a measure checking last month values never changes. To do that you can take advantage of the Refresh_Check and Refresh_Check_Collection classes (Sorry, I know the documentation stinks right now). But using those you can build out something that would first check the results of the measure, then refresh, then check the results of the measure after refresh, and lastly perform your desired check. In this case the pre value matches the post value. When refreshing and your pre does not equal post, it would fail and give an assertion error in your logging. from pytabular import Tabular from pytabular.refresh import RefreshCheck, RefreshCheckCollection model = Tabular(CONNECTION_STR) # This is our custom check that we want to run after refresh. # Does the pre refresh value match the post refresh value. def sum_of_sales_assertion(pre, post): return pre == post # This is where we put it all together into the `Refresh_Check` class. Give it a name, give it a query to run, and give it the assertion you want to make. sum_of_last_month_sales = RefreshCheck( 'Last Month Sales', lambda: model.query(\"EVALUATE {[Last Month Sales]}\") ,sum_of_sales_assertion ) # Here we are adding it to a `Refresh_Check_Collection` because you can have more than on `Refresh_Check` to run. all_refresh_check = RefreshCheckCollection([sum_of_last_month_sales]) model.Refresh( 'Fact Table Name', refresh_checks = RefreshCheckCollection([sum_of_last_month_sales]) ) Query as Another User There are plenty of tools that allow you to query as an 'Effective User' inheriting their security when querying. This is an extremely valuable concept built natively into the .Net apis. My only gripe is they were all UI based. This allows you to programmatically connect as an effective user and query in Python. You could easily loop through all your users to run tests on their security. import pytabular as p #Connect to your model like usual... model = p.Tabular(CONNECTION_STR) #This will be the query I run... query_str = ''' EVALUATE SUMMARIZE( 'Product Dimension', 'Product Dimension'[Product Name], \"Total Product Sales\", [Total Sales] ) ''' #This will be the user I want to query as... user_email = 'user1@company.com' #Base line, to query as the user connecting to the model. model.query(query_str) #Option 1, Connect via connection class... user1 = p.Connection(model.Server, Effective_User = user_email) user1.query(query_str) #Option 2, Just add Effective_User model.query(query_str, Effective_User = user_email) #PyTabular will do it's best to handle multiple accounts... #So you won't have to reconnect on every query Refresh Related Tables Ever need to refresh related tables of a Fact? Now should be a lot easier. import pytabular as p #Connect to model model = p.Tabular(CONNECTION_STR) #Get related tables tables = model.Tables[TABLE_NAME].related() #Now just refresh like usual... tables.refresh() Documenting a Model The Tabular model contains a lot of information that can be used to generation documentation if filled in. Currently the markdown files are generated with the Docusaurs heading in place, but this will be changed in future to support multiple documentation platforms. Tip : With Tabular Editor 2 (Free) or 3 (Paid) you can easily add Descriptioms, Translations (Cultures) and other additonal information that can later be used for generating the documentation. Args: - model : Tabular - friendly_name : Default > No Value To specify the location of the docs, just supply the save location with a new folder name argument. - save_location : Default > docs Each page in the generation process has it's own specific name, with these arguments you can rename them to your liking. - general_page_url : Default > 1-general-information.md - measure_page_url : Default > 2-measures.md - table_page_url : Default > 3-tables.md - column_page_url : Default > 4-columns.md - roles_page_url : Default > 5-roles.md Documenting a Model The simpelst way to document a tabular model is to connect to the model, and initialize the documentation and execute save_documentation() . import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs docs = pytabular.ModelDocumenter(model) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation() Documenting a Model with Cultures Some model creators choose to add cultures to a tabular model for different kinds of reasons. We can leverage those cultures to use the translation names instead of the original object names. In order to this you can set translations to True and specify the culture you want to use (e.g. 'en-US' ). import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs docs = pytabular.ModelDocumenter(model) # Set the translation for documentation to an available culture. # By setting the Tranlsations to `True` it will check if it exists and if it does, # it will start using the translations for the docs docs.set_translations( enable_translations = True, culture = 'en-US' ) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation() Documenting a Power BI > Local Model. The Local model doesn't have a \"name\", only an Id. So we need to Supply a \"Friendly Name\", which will be used to store the markdown files. import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs and set a friendly name to store the markdown files. docs = pytabular.ModelDocumenter( model = model, friendly_name = \"Adventure Works\" ) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation() Contributing See contributing.md","title":"Home"},{"location":"#pytabular","text":"","title":"PyTabular"},{"location":"#what-is-it","text":"PyTabular (python-tabular in pypi ) is a python package that allows for programmatic execution on your tabular models! This is possible thanks to Pythonnet and Microsoft's .Net APIs on Azure Analysis Services . Currently, this build is tested and working on Windows Operating System only. Help is needed to expand this for other operating systems. The package should have the dll files included when you import it. See Documentation Here . PyTabular is still considered alpha while I'm working on building out the proper tests and testing environments, so I can ensure some kind of stability in features. Please send bugs my way! Preferably in the issues section in Github. I want to harden this project so many can use it easily. I currently have local pytest for python 3.6 to 3.10 and run those tests through a local AAS and Gen2 model.","title":"What is it?"},{"location":"#getting-started","text":"See the Pypi project for available versions. To become PEP8 compliant with naming conventions, serious name changes were made in 0.3.5. Install v. 0.3.4 or lower to get the older naming conventions. python3 -m pip install python-tabular #install specific version python3 -m pip install python-tabular==0.3.4 In your python environment, import pytabular and call the main Tabular Class. Only parameter needed is a solid connection string. import pytabular model = pytabular.Tabular(CONNECTION_STR) I'm a big fan of logging, if you don't want any just get the logger and disable it. import pytabular pytabular.logger.disabled = True You can query your models with the Query method from your tabular class. For Dax Queries, it will need the full Dax syntax. See EVALUATE example . This will return a Pandas DataFrame . If you are looking to return a single value, see below. Simply wrap your query in the the curly brackets. The method will take that single cell table and just return the individual value. You can also query your DMV. See below for example. See PyTabular Docs for Query . #Run basic queries DAX_QUERY = \"EVALUATE TOPN(100, 'Table1')\" model.query(DAX_QUERY) #returns pd.DataFrame() #or... DMV_QUERY = \"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\" model.query(DMV_QUERY) #returns pd.DataFrame() #or... SINGLE_VALUE_QUERY_EX = \"EVALUATE {1}\" model.query(SINGLE_VALUE_QUERY_EX) #returns 1 #or... FILE_PATH = 'C:\\\\FILEPATHEXAMPLE\\\\file.dax' #or file.txt model.query(FILE_PATH) #Will return same logic as above, single values if possible else will return pd.DataFrame() You can also explore your tables, partitions, and columns. Via the Attributes from your Tabular class. #Explore tables... dir(model.Tables['Table Name']) #Explore columns & partitions dir(model.Tables['Table Name'].Partitions['Partition Name']) #Only a few features right now, but check out the built in methods. model.Tables['Table Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].last_refresh() #or model.Tables['Table Name'].row_count() #or model.Tables['Table Name'].Columns['Column Name'].distinct_count() Refresh method to handle refreshes on your model. This is synchronous. Should be flexible enough to handle a variety of inputs. See PyTabular Docs for Refreshing Tables and Partitions . Most basic way to refresh is input the table name string. The method will search for table and output exeption if unable to find it. For partitions you will need a key, value combination. Example, {'Table1':'Partition1'} . You can also take the key value pair and iterate through a group of partitions. Example, {'Table1':['Partition1','Partition2']} . Rather than providing a string, you can also input the actual class. See below for those examples, and you can acess them from the built in attributes self.Tables , self.Partitions or explore through the .Net classes yourself in self.Model.Tables . #You have a few options when refreshing. model.refresh('Table Name') #or... model.refresh(['Table1','Table2','Table3']) #or... model.refresh(<Table Class>) #or... model.refresh(<Partition Class>) #or... model.refresh({'Table Name':'Partition Name'}) #or any kind of weird combination like model.refresh([{<Table Class>:<Partition Class>,'Table Name':['Partition1','Partition2']},'Table Name','Table Name2']) #You can even run through the Tables & Partition Attributes model.Tables['Table Name'].refresh() #or model.Tables['Table Name'].Partitions['Partition Name'].refresh() #Default Tracing happens automatically, but can be removed by... model.refresh(['Table1','Table2'], trace = None) It's not uncommon to need to run through some checks on specific Tables, Partitions, Columns, Etc... #Get Row Count from model model.Tables['Table Name'].row_count() #Get Last Refresh time from a partition model.Tables['Table Name'].last_refresh() #Get Distinct Count or Values from a Column model.Tables['Table Name'].Columns['Column Name'].distinct_count() #or model.Tables['Table Name'].Columns['Column Name'].values()","title":"Getting Started"},{"location":"#use-cases","text":"","title":"Use Cases"},{"location":"#if-blank-table-then-refresh-table","text":"This will use the function Return_Zero_Row_Tables and the method Refresh from the Tabular class. import pytabular model = pytabular.Tabular(CONNECTION_STR) tables = model.Tables.find_zero_rows() if len(tables) > 0: model.refresh(tables)","title":"If blank table, then refresh table."},{"location":"#sneak-in-a-refresh","text":"This will use the method Is_Process and the method Refresh from the Tabular class. It will check the DMV to see if any jobs are currently running classified as processing. import pytabular model = pytabular.Tabular(CONNECTION_STR) if model.is_process(): #do what you want if there is a refresh happening else: model.refresh(TABLES_OR_PARTITIONS_TO_REFRESH)","title":"Sneak in a refresh."},{"location":"#show-refresh-times-in-model","text":"This will use the function Table_Last_Refresh_Times and the method Create_Table from the Tabular class. It will search through the model for all tables and partitions and pull the 'RefreshedTime' property from it. It will return results into a pandas dataframe, which will then be converted into an M expression used for a new table. import pytabular model = pytabular.Tabular(CONNECTION_STR) df = model.Tables.last_refresh() model.create_table(df, 'Refresh Times')","title":"Show refresh times in model."},{"location":"#if-bpa-violation-then-revert-deployment","text":"Uses a few things. First the BPA Class , then the TE2 Class , and will finish with the Analyze_BPA method. Did not want to re-invent the wheel with the amazing work done with Tabular Editor and it's BPA capabilities. import pytabular model = pytabular.Tabular(CONNECTION_STR) te2 = pytabular.TabularEditor() #Feel free to input your TE2 File path or this will download for you. bpa = pytabular.BPA() #Fee free to input your own BPA file or this will download for you from: https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json results = model.analyze_bpa(te2.exe,bpa.location) if len(results) > 0: #Revert deployment here!","title":"If BPA Violation, then revert deployment."},{"location":"#loop-through-and-query-dax-files","text":"Let's say you have multiple dax queries you would like to store and run through as checks. The Query method on the Tabular class can also take file paths. Can really be any file type as it's just checking os.path.isfile(). But would suggest .dax or .txt. It will read the file that use that as the new Query_str argument. import pytabular model = pytabular.Tabular(CONNECTION_STR) LIST_OF_FILE_PATHS = ['C:\\\\FilePath\\\\file1.dax','C:\\\\FilePath\\\\file1.txt','C:\\\\FilePath\\\\file2.dax','C:\\\\FilePath\\\\file2.txt'] for file_path in LIST_OF_FILE_PATHS: model.query(file_path)","title":"Loop through and query Dax files"},{"location":"#advanced-refreshing-with-pre-and-post-checks","text":"Maybe you are introducing new logic to a fact table, and you need to ensure that a measure checking last month values never changes. To do that you can take advantage of the Refresh_Check and Refresh_Check_Collection classes (Sorry, I know the documentation stinks right now). But using those you can build out something that would first check the results of the measure, then refresh, then check the results of the measure after refresh, and lastly perform your desired check. In this case the pre value matches the post value. When refreshing and your pre does not equal post, it would fail and give an assertion error in your logging. from pytabular import Tabular from pytabular.refresh import RefreshCheck, RefreshCheckCollection model = Tabular(CONNECTION_STR) # This is our custom check that we want to run after refresh. # Does the pre refresh value match the post refresh value. def sum_of_sales_assertion(pre, post): return pre == post # This is where we put it all together into the `Refresh_Check` class. Give it a name, give it a query to run, and give it the assertion you want to make. sum_of_last_month_sales = RefreshCheck( 'Last Month Sales', lambda: model.query(\"EVALUATE {[Last Month Sales]}\") ,sum_of_sales_assertion ) # Here we are adding it to a `Refresh_Check_Collection` because you can have more than on `Refresh_Check` to run. all_refresh_check = RefreshCheckCollection([sum_of_last_month_sales]) model.Refresh( 'Fact Table Name', refresh_checks = RefreshCheckCollection([sum_of_last_month_sales]) )","title":"Advanced Refreshing with Pre and Post Checks"},{"location":"#query-as-another-user","text":"There are plenty of tools that allow you to query as an 'Effective User' inheriting their security when querying. This is an extremely valuable concept built natively into the .Net apis. My only gripe is they were all UI based. This allows you to programmatically connect as an effective user and query in Python. You could easily loop through all your users to run tests on their security. import pytabular as p #Connect to your model like usual... model = p.Tabular(CONNECTION_STR) #This will be the query I run... query_str = ''' EVALUATE SUMMARIZE( 'Product Dimension', 'Product Dimension'[Product Name], \"Total Product Sales\", [Total Sales] ) ''' #This will be the user I want to query as... user_email = 'user1@company.com' #Base line, to query as the user connecting to the model. model.query(query_str) #Option 1, Connect via connection class... user1 = p.Connection(model.Server, Effective_User = user_email) user1.query(query_str) #Option 2, Just add Effective_User model.query(query_str, Effective_User = user_email) #PyTabular will do it's best to handle multiple accounts... #So you won't have to reconnect on every query","title":"Query as Another User"},{"location":"#refresh-related-tables","text":"Ever need to refresh related tables of a Fact? Now should be a lot easier. import pytabular as p #Connect to model model = p.Tabular(CONNECTION_STR) #Get related tables tables = model.Tables[TABLE_NAME].related() #Now just refresh like usual... tables.refresh()","title":"Refresh Related Tables"},{"location":"#documenting-a-model","text":"The Tabular model contains a lot of information that can be used to generation documentation if filled in. Currently the markdown files are generated with the Docusaurs heading in place, but this will be changed in future to support multiple documentation platforms. Tip : With Tabular Editor 2 (Free) or 3 (Paid) you can easily add Descriptioms, Translations (Cultures) and other additonal information that can later be used for generating the documentation. Args: - model : Tabular - friendly_name : Default > No Value To specify the location of the docs, just supply the save location with a new folder name argument. - save_location : Default > docs Each page in the generation process has it's own specific name, with these arguments you can rename them to your liking. - general_page_url : Default > 1-general-information.md - measure_page_url : Default > 2-measures.md - table_page_url : Default > 3-tables.md - column_page_url : Default > 4-columns.md - roles_page_url : Default > 5-roles.md","title":"Documenting a Model"},{"location":"#documenting-a-model_1","text":"The simpelst way to document a tabular model is to connect to the model, and initialize the documentation and execute save_documentation() . import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs docs = pytabular.ModelDocumenter(model) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation()","title":"Documenting a Model"},{"location":"#documenting-a-model-with-cultures","text":"Some model creators choose to add cultures to a tabular model for different kinds of reasons. We can leverage those cultures to use the translation names instead of the original object names. In order to this you can set translations to True and specify the culture you want to use (e.g. 'en-US' ). import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs docs = pytabular.ModelDocumenter(model) # Set the translation for documentation to an available culture. # By setting the Tranlsations to `True` it will check if it exists and if it does, # it will start using the translations for the docs docs.set_translations( enable_translations = True, culture = 'en-US' ) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation()","title":"Documenting a Model with Cultures"},{"location":"#documenting-a-power-bi-local-model","text":"The Local model doesn't have a \"name\", only an Id. So we need to Supply a \"Friendly Name\", which will be used to store the markdown files. import pytabular # Connect to a Tabular Model Model model = pytabular.Tabular(CONNECTION_STR) # Initiate the Docs and set a friendly name to store the markdown files. docs = pytabular.ModelDocumenter( model = model, friendly_name = \"Adventure Works\" ) # Generate the pages. docs.generate_documentation_pages() # Save docs to the default location docs.save_documentation()","title":"Documenting a Power BI &gt; Local Model."},{"location":"#contributing","text":"See contributing.md","title":"Contributing"},{"location":"Best%20Practice%20Analyzer/","text":"BPA source BPA( file_path: str = 'Default' ) Setting BPA Class for future work... download_bpa_file source .download_bpa_file( download_location: str = 'https: //raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json', folder: str = 'Best_Practice_Analyzer', auto_remove = True ) Download a BPA file from local or web. Runs a request.get() to retrieve the json file from web. Will return and store in directory. Will also register the removal of the new directory and file when exiting program. Args download_location (str, optional) : Defaults to [BPA]'https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json'. folder (str, optional) : New folder string. Defaults to 'Best_Practice_Analyzer'. auto_remove (bool, optional) : Auto Remove when script exits. Defaults to True. Returns str : File path for the newly downloaded BPA.","title":"Best Practice Analyzer"},{"location":"Best%20Practice%20Analyzer/#_1","text":"","title":""},{"location":"Best%20Practice%20Analyzer/#bpa","text":"source BPA( file_path: str = 'Default' ) Setting BPA Class for future work...","title":"BPA"},{"location":"Best%20Practice%20Analyzer/#download_bpa_file","text":"source .download_bpa_file( download_location: str = 'https: //raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json', folder: str = 'Best_Practice_Analyzer', auto_remove = True ) Download a BPA file from local or web. Runs a request.get() to retrieve the json file from web. Will return and store in directory. Will also register the removal of the new directory and file when exiting program. Args download_location (str, optional) : Defaults to [BPA]'https://raw.githubusercontent.com/microsoft/Analysis-Services/master/BestPracticeRules/BPARules.json'. folder (str, optional) : New folder string. Defaults to 'Best_Practice_Analyzer'. auto_remove (bool, optional) : Auto Remove when script exits. Defaults to True. Returns str : File path for the newly downloaded BPA.","title":"download_bpa_file"},{"location":"Column/","text":"PyColumn source PyColumn( object, table ) The main class to work with your columns. Notice the PyObject magic method __getattr__() will search in self._object if it is unable to find it in the default attributes. This let's you also easily check the default .Net properties. See methods for extra functionality. Args Table : Parent table to the column. Methods: .distinct_count source .distinct_count( no_blank = False ) Get the DISTINCTCOUNT of a column. Args no_blank (bool, optional) : If True , will call DISTINCTCOUNTNOBLANK . Defaults to False . Returns int : Number of Distinct Count from column. If no_blank == True then will return number of distinct count no blanks. .get_dependencies source .get_dependencies() Returns the dependant columns of a measure. .values source .values() Get single column DataFrame of values in column. Similar to get_sample_values() but will return all . Returns DataFrame : Single column DataFrame of values. . init source .__init__( object, table ) Init that connects your column to parent table. It will also build custom rows for your rich display table. Args object (Column) : .Net column object. table (Table) : .Net table object. .get_sample_values source .get_sample_values( top_n: int = 3 ) Get sample values of column. PyColumns source PyColumns( objects ) Groups together multiple PyColumn() . See PyObjects class for what more it can do. You can interact with PyColumns straight from model. For ex: model.Columns . Or through individual tables model.Tables[TABLE_NAME].Columns . You can even filter down with .Find() . For example find all columns with Key in name. model.Columns.Find('Key') . Methods: . init source .__init__( objects ) Init extends through to the PyObjects() init. .query_all source .query_all( query_function: str = 'COUNTROWS(VALUES(_))' ) This will dynamically all columns in PyColumns() class. It will replace the _ with the column to run whatever the given query_function value is. Args query_function (str, optional) : Default is COUNTROWS(VALUES(_)) . The _ gets replaced with the column in question. Method will take whatever DAX query is given. Returns DataFrame : Returns dataframe with results.","title":"Columns"},{"location":"Column/#_1","text":"","title":""},{"location":"Column/#pycolumn","text":"source PyColumn( object, table ) The main class to work with your columns. Notice the PyObject magic method __getattr__() will search in self._object if it is unable to find it in the default attributes. This let's you also easily check the default .Net properties. See methods for extra functionality. Args Table : Parent table to the column. Methods:","title":"PyColumn"},{"location":"Column/#distinct_count","text":"source .distinct_count( no_blank = False ) Get the DISTINCTCOUNT of a column. Args no_blank (bool, optional) : If True , will call DISTINCTCOUNTNOBLANK . Defaults to False . Returns int : Number of Distinct Count from column. If no_blank == True then will return number of distinct count no blanks.","title":".distinct_count"},{"location":"Column/#get_dependencies","text":"source .get_dependencies() Returns the dependant columns of a measure.","title":".get_dependencies"},{"location":"Column/#values","text":"source .values() Get single column DataFrame of values in column. Similar to get_sample_values() but will return all . Returns DataFrame : Single column DataFrame of values.","title":".values"},{"location":"Column/#init","text":"source .__init__( object, table ) Init that connects your column to parent table. It will also build custom rows for your rich display table. Args object (Column) : .Net column object. table (Table) : .Net table object.","title":".init"},{"location":"Column/#get_sample_values","text":"source .get_sample_values( top_n: int = 3 ) Get sample values of column.","title":".get_sample_values"},{"location":"Column/#pycolumns","text":"source PyColumns( objects ) Groups together multiple PyColumn() . See PyObjects class for what more it can do. You can interact with PyColumns straight from model. For ex: model.Columns . Or through individual tables model.Tables[TABLE_NAME].Columns . You can even filter down with .Find() . For example find all columns with Key in name. model.Columns.Find('Key') . Methods:","title":"PyColumns"},{"location":"Column/#init_1","text":"source .__init__( objects ) Init extends through to the PyObjects() init.","title":".init"},{"location":"Column/#query_all","text":"source .query_all( query_function: str = 'COUNTROWS(VALUES(_))' ) This will dynamically all columns in PyColumns() class. It will replace the _ with the column to run whatever the given query_function value is. Args query_function (str, optional) : Default is COUNTROWS(VALUES(_)) . The _ gets replaced with the column in question. Method will take whatever DAX query is given. Returns DataFrame : Returns dataframe with results.","title":".query_all"},{"location":"Logic%20Utils/","text":"ticks_to_datetime source .ticks_to_datetime( ticks: int ) Converts a C# system datetime tick into a python datatime. Args ticks (int) : C# DateTime Tick. Returns datetime : datetime of tick. pandas_datatype_to_tabular_datatype source .pandas_datatype_to_tabular_datatype( df: pd.DataFrame ) Takes dataframe columns and gets respective tabular column datatype. Args df (pd.DataFrame) : Pandas DataFrame Returns Dict : dictionary with results. Example , 'col2': , 'col3': { } pd_dataframe_to_m_expression source .pd_dataframe_to_m_expression( df: pd.DataFrame ) This will take a pandas dataframe and convert to an m expression. Args df (pd.DataFrame) : Pandas DataFrame Returns str : Currently only returning string values in your tabular model. Example col1 col2 0 1 3 1 2 4 converts to Source=#table({\"col1\",\"col2\"}, { {\"1\",\"3\"},{\"2\",\"4\"} }) in Source remove_folder_and_contents source .remove_folder_and_contents( folder_location ) Internal used in tabular_editor.py and best_practice_analyzer.py. Args folder_location (str) : Folder path to remove directory and contents. remove_suffix source .remove_suffix( input_string, suffix ) Adding for <3.9 compatiblity. Args input_string (str) : input string to remove suffix from. suffix (str) : suffix to be removed. Returns str : input_str with suffix removed. remove_file source .remove_file( file_path ) Just os.remove() but wanted a logger.info() with it. Args file_path : See os.remove get_sub_list source .get_sub_list( lst: list, n: int ) Nest list by n amount. Args lst (list) : List to nest. n (int) : Amount to nest list. Returns list : Nested list. Example get_sub_list([1,2,3,4,5,6],2) == [[1,2],[3,4],[5,6]] get_value_to_df source .get_value_to_df( query: AdomdDataReader, index: int ) Gets the values from the AdomdDataReader to convert to python df. Lots of room for improvement on this one. Args query (AdomdDataReader) : The AdomdDataReader .Net object. index (int) : Index of the value to perform the logic on. dataframe_to_dict source .dataframe_to_dict( df: pd.DataFrame ) Convert to Dataframe to dictionary and alter columns names with it. Will convert the underscores (_) to spaces, and all strings are converted to Title Case. Args df (pd.DataFrame) : Original table that needs to be converted to a list with dicts. Returns list of dictionaries. dict_to_markdown_table source .dict_to_markdown_table( list_of_dicts: list, columns_to_include: list = None ) Generate a Markdown table based on a list of dictionaries. Args list_of_dicts (list) : List of Dictionaries that need to be converted to a markdown table. columns_to_include (list) : Default = None, and all colums are included. If a list is supplied, those columns will be included. Returns String that will represent a table in Markdown. Example columns = ['Referenced Object Type', 'Referenced Table', 'Referenced Object'] dict_to_markdown_table(dependancies, columns) Returns | Referenced Object Type | Referenced Table | Referenced Object | | ---------------------- | ---------------- | ------------------------------- | | TABLE | Cases | Cases | | COLUMN | Cases | IsClosed | | CALC_COLUMN | Cases | Resolution Time (Working Hours) |","title":"Logic Utils"},{"location":"Logic%20Utils/#_1","text":"","title":""},{"location":"Logic%20Utils/#ticks_to_datetime","text":"source .ticks_to_datetime( ticks: int ) Converts a C# system datetime tick into a python datatime. Args ticks (int) : C# DateTime Tick. Returns datetime : datetime of tick.","title":"ticks_to_datetime"},{"location":"Logic%20Utils/#pandas_datatype_to_tabular_datatype","text":"source .pandas_datatype_to_tabular_datatype( df: pd.DataFrame ) Takes dataframe columns and gets respective tabular column datatype. Args df (pd.DataFrame) : Pandas DataFrame Returns Dict : dictionary with results. Example , 'col2': , 'col3': { }","title":"pandas_datatype_to_tabular_datatype"},{"location":"Logic%20Utils/#pd_dataframe_to_m_expression","text":"source .pd_dataframe_to_m_expression( df: pd.DataFrame ) This will take a pandas dataframe and convert to an m expression. Args df (pd.DataFrame) : Pandas DataFrame Returns str : Currently only returning string values in your tabular model. Example col1 col2 0 1 3 1 2 4 converts to Source=#table({\"col1\",\"col2\"}, { {\"1\",\"3\"},{\"2\",\"4\"} }) in Source","title":"pd_dataframe_to_m_expression"},{"location":"Logic%20Utils/#remove_folder_and_contents","text":"source .remove_folder_and_contents( folder_location ) Internal used in tabular_editor.py and best_practice_analyzer.py. Args folder_location (str) : Folder path to remove directory and contents.","title":"remove_folder_and_contents"},{"location":"Logic%20Utils/#remove_suffix","text":"source .remove_suffix( input_string, suffix ) Adding for <3.9 compatiblity. Args input_string (str) : input string to remove suffix from. suffix (str) : suffix to be removed. Returns str : input_str with suffix removed.","title":"remove_suffix"},{"location":"Logic%20Utils/#remove_file","text":"source .remove_file( file_path ) Just os.remove() but wanted a logger.info() with it. Args file_path : See os.remove","title":"remove_file"},{"location":"Logic%20Utils/#get_sub_list","text":"source .get_sub_list( lst: list, n: int ) Nest list by n amount. Args lst (list) : List to nest. n (int) : Amount to nest list. Returns list : Nested list. Example get_sub_list([1,2,3,4,5,6],2) == [[1,2],[3,4],[5,6]]","title":"get_sub_list"},{"location":"Logic%20Utils/#get_value_to_df","text":"source .get_value_to_df( query: AdomdDataReader, index: int ) Gets the values from the AdomdDataReader to convert to python df. Lots of room for improvement on this one. Args query (AdomdDataReader) : The AdomdDataReader .Net object. index (int) : Index of the value to perform the logic on.","title":"get_value_to_df"},{"location":"Logic%20Utils/#dataframe_to_dict","text":"source .dataframe_to_dict( df: pd.DataFrame ) Convert to Dataframe to dictionary and alter columns names with it. Will convert the underscores (_) to spaces, and all strings are converted to Title Case. Args df (pd.DataFrame) : Original table that needs to be converted to a list with dicts. Returns list of dictionaries.","title":"dataframe_to_dict"},{"location":"Logic%20Utils/#dict_to_markdown_table","text":"source .dict_to_markdown_table( list_of_dicts: list, columns_to_include: list = None ) Generate a Markdown table based on a list of dictionaries. Args list_of_dicts (list) : List of Dictionaries that need to be converted to a markdown table. columns_to_include (list) : Default = None, and all colums are included. If a list is supplied, those columns will be included. Returns String that will represent a table in Markdown. Example columns = ['Referenced Object Type', 'Referenced Table', 'Referenced Object'] dict_to_markdown_table(dependancies, columns) Returns | Referenced Object Type | Referenced Table | Referenced Object | | ---------------------- | ---------------- | ------------------------------- | | TABLE | Cases | Cases | | COLUMN | Cases | IsClosed | | CALC_COLUMN | Cases | Resolution Time (Working Hours) |","title":"dict_to_markdown_table"},{"location":"Measure/","text":"PyMeasure source PyMeasure( object, table ) Main class for interacting with measures. See methods for available functionality. Methods: . init source .__init__( object, table ) Connects measure to parent PyTable . It will also add some custom rows for the rich table display. Args object : The .Net measure object. table (PyTable) : The parent PyTable . .get_dependencies source .get_dependencies() Returns the dependant objects of a measure. Args self : The Measure Object Returns DataFrame : The Return Value is a Pandas dataframe which displays all the dependancies of the object. PyMeasures source PyMeasures( objects ) Groups together multiple measures. See PyObjects class for what more it can do. You can interact with PyMeasures straight from model. For ex: model.Measures . Or through individual tables model.Tables[TABLE_NAME].Measures . You can even filter down with .find() . For example find all measures with ratio in name. model.Measures.find('ratio') . Methods: . init source .__init__( objects ) Extends init from PyObjects .","title":"Measures"},{"location":"Measure/#_1","text":"","title":""},{"location":"Measure/#pymeasure","text":"source PyMeasure( object, table ) Main class for interacting with measures. See methods for available functionality. Methods:","title":"PyMeasure"},{"location":"Measure/#init","text":"source .__init__( object, table ) Connects measure to parent PyTable . It will also add some custom rows for the rich table display. Args object : The .Net measure object. table (PyTable) : The parent PyTable .","title":".init"},{"location":"Measure/#get_dependencies","text":"source .get_dependencies() Returns the dependant objects of a measure. Args self : The Measure Object Returns DataFrame : The Return Value is a Pandas dataframe which displays all the dependancies of the object.","title":".get_dependencies"},{"location":"Measure/#pymeasures","text":"source PyMeasures( objects ) Groups together multiple measures. See PyObjects class for what more it can do. You can interact with PyMeasures straight from model. For ex: model.Measures . Or through individual tables model.Tables[TABLE_NAME].Measures . You can even filter down with .find() . For example find all measures with ratio in name. model.Measures.find('ratio') . Methods:","title":"PyMeasures"},{"location":"Measure/#init_1","text":"source .__init__( objects ) Extends init from PyObjects .","title":".init"},{"location":"PBI%20Helper/","text":"find_local_pbi_instances source .find_local_pbi_instances() The real genius is from Dax Studio. I just wanted it in python not C#, so reverse engineered what DaxStudio did. It will run some powershell scripts to pull the appropriate info. Then will spit out a list with tuples inside. You can use the connection string to connect to your model with pytabular. Dax Studio . Returns list : EX [('PBI File Name1','localhost:{port}'),('PBI File Name2','localhost:{port}')]","title":"PBI Helper"},{"location":"PBI%20Helper/#_1","text":"","title":""},{"location":"PBI%20Helper/#find_local_pbi_instances","text":"source .find_local_pbi_instances() The real genius is from Dax Studio. I just wanted it in python not C#, so reverse engineered what DaxStudio did. It will run some powershell scripts to pull the appropriate info. Then will spit out a list with tuples inside. You can use the connection string to connect to your model with pytabular. Dax Studio . Returns list : EX [('PBI File Name1','localhost:{port}'),('PBI File Name2','localhost:{port}')]","title":"find_local_pbi_instances"},{"location":"Partition/","text":"PyPartition source PyPartition( object, table ) Main class for interacting with partitions. See methods for available uses. Methods: .refresh source .refresh( *args, **kwargs ) Same method from Model Refresh. You can pass through any extra parameters. For example: Tabular().Tables['Table Name'].Partitions[0].refresh() Returns DataFrame : Returns pandas dataframe with some refresh details . init source .__init__( object, table ) Extends from PyObject class. Adds a few custom rows to rich table for the partition. Args object (Partition) : .Net Partition object. table (PyTable) : Parent table of the partition in question. .last_refresh source .last_refresh() Queries RefreshedTime attribute in the partition. Converts from C# Ticks to Python datetime. Returns datetime : Last Refreshed time of Partition in datetime format PyPartitions source PyPartitions( objects ) Groups together multiple partitions. See PyObjects class for what more it can do. You can interact with PyPartitions straight from model. For ex: model.Partitions . Or through individual tables model.Tables[TABLE_NAME].Partitions . You can even filter down with .find() . For example find partitions with prev-year in name. model.Partitions.find('prev-year') . Methods: .refresh source .refresh( *args, **kwargs ) Refreshes all PyPartition (s) in class. . init source .__init__( objects ) Extends through to PyObjects .","title":"Partitions"},{"location":"Partition/#_1","text":"","title":""},{"location":"Partition/#pypartition","text":"source PyPartition( object, table ) Main class for interacting with partitions. See methods for available uses. Methods:","title":"PyPartition"},{"location":"Partition/#refresh","text":"source .refresh( *args, **kwargs ) Same method from Model Refresh. You can pass through any extra parameters. For example: Tabular().Tables['Table Name'].Partitions[0].refresh() Returns DataFrame : Returns pandas dataframe with some refresh details","title":".refresh"},{"location":"Partition/#init","text":"source .__init__( object, table ) Extends from PyObject class. Adds a few custom rows to rich table for the partition. Args object (Partition) : .Net Partition object. table (PyTable) : Parent table of the partition in question.","title":".init"},{"location":"Partition/#last_refresh","text":"source .last_refresh() Queries RefreshedTime attribute in the partition. Converts from C# Ticks to Python datetime. Returns datetime : Last Refreshed time of Partition in datetime format","title":".last_refresh"},{"location":"Partition/#pypartitions","text":"source PyPartitions( objects ) Groups together multiple partitions. See PyObjects class for what more it can do. You can interact with PyPartitions straight from model. For ex: model.Partitions . Or through individual tables model.Tables[TABLE_NAME].Partitions . You can even filter down with .find() . For example find partitions with prev-year in name. model.Partitions.find('prev-year') . Methods:","title":"PyPartitions"},{"location":"Partition/#refresh_1","text":"source .refresh( *args, **kwargs ) Refreshes all PyPartition (s) in class.","title":".refresh"},{"location":"Partition/#init_1","text":"source .__init__( objects ) Extends through to PyObjects .","title":".init"},{"location":"Queries/","text":"Connection source Connection( server, effective_user = None ) Connection class creates an AdomdConnection. Mainly used for the query() method. The query() method in the Tabular class is just a wrapper for this class. But you can pass through your effective_user more efficiently, so use that instead. Methods: . init source .__init__( server, effective_user = None ) Init creates the connection. Args server (Server) : The server that you are connecting to. effective_user (str, optional) : Pass through an effective user to query as somebody else. Defaults to None. .query source .query( query_str: str ) Executes query on Model and returns results in Pandas DataFrame. Iterates through results of AdomdCommmand().ExecuteReader() in the .Net library. If result is a single value, it will return that single value instead of DataFrame. Args query_str (str) : Dax Query. Note, needs full syntax (ex: EVALUATE ). See (DAX)[https://docs.microsoft.com/en-us/dax/dax-queries]. Will check if query string is a file. If it is, then it will perform a query on whatever is read from the file. It is also possible to query DMV. For example. query(\"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\") . Returns DataFrame : Returns dataframe with results.","title":"Querying"},{"location":"Queries/#_1","text":"","title":""},{"location":"Queries/#connection","text":"source Connection( server, effective_user = None ) Connection class creates an AdomdConnection. Mainly used for the query() method. The query() method in the Tabular class is just a wrapper for this class. But you can pass through your effective_user more efficiently, so use that instead. Methods:","title":"Connection"},{"location":"Queries/#init","text":"source .__init__( server, effective_user = None ) Init creates the connection. Args server (Server) : The server that you are connecting to. effective_user (str, optional) : Pass through an effective user to query as somebody else. Defaults to None.","title":".init"},{"location":"Queries/#query","text":"source .query( query_str: str ) Executes query on Model and returns results in Pandas DataFrame. Iterates through results of AdomdCommmand().ExecuteReader() in the .Net library. If result is a single value, it will return that single value instead of DataFrame. Args query_str (str) : Dax Query. Note, needs full syntax (ex: EVALUATE ). See (DAX)[https://docs.microsoft.com/en-us/dax/dax-queries]. Will check if query string is a file. If it is, then it will perform a query on whatever is read from the file. It is also possible to query DMV. For example. query(\"select * from $SYSTEM.DISCOVER_TRACE_EVENT_CATEGORIES\") . Returns DataFrame : Returns dataframe with results.","title":".query"},{"location":"Refreshes/","text":"PyRefresh source PyRefresh( model, object: Union[str, PyTable, PyPartition, Dict[str, Any]], trace: BaseTrace = RefreshTrace, refresh_checks: RefreshCheckCollection = RefreshCheckCollection(), default_row_count_check: bool = True, refresh_type: RefreshType = RefreshType.Full ) PyRefresh Class to handle refreshes of model. Methods: ._get_trace source ._get_trace() Creates Trace and creates it in model. ._find_partition source ._find_partition( table: Table, partition_str: str ) Finds partition in PyPartitions class. ._refresh_table source ._refresh_table( table: PyTable ) Runs .Net RequestRefresh() on table. ._refresh_dict source ._refresh_dict( partition_dict: Dict ) Handles refreshes if argument given was a dictionary. ._find_table source ._find_table( table_str: str ) Finds table in PyTables class. ._request_refresh source ._request_refresh( object ) Base method to parse through argument and figure out what needs to be refreshed. Someone please make this better... ._pre_checks source ._pre_checks() Checks if any BaseTrace classes are needed from tabular_tracing.py . Then checks if any RefreshChecks are needed, along with the default row_count check. ._post_checks source ._post_checks() If traces are running it Stops and Drops it. Runs through any post_checks() in RefreshChecks . .run source .run() When ready, execute to start the refresh process. First checks if connected and reconnects if needed. Then starts the trace if needed. Next will execute save_changes() and run the post checks after that. Last will return a pd.DataFrame of refresh results. . init source .__init__( model, object: Union[str, PyTable, PyPartition, Dict[str, Any]], trace: BaseTrace = RefreshTrace, refresh_checks: RefreshCheckCollection = RefreshCheckCollection(), default_row_count_check: bool = True, refresh_type: RefreshType = RefreshType.Full ) Init when a refresh is requested. Runs through requested tables and partitions to make sure they are in model. Then will run pre checks on the requested objects. Args model (Tabular) : Tabular model. object (Union[str, PyTable, PyPartition, Dict[str, Any]]) : The objects that you are wanting to refresh. Can be a PyTable , PyPartition , TABLE_NAME string, or a dict with {TABLE_REFERENCE:PARTITION_REFERENCE} trace (BaseTrace, optional) : Defaults to RefreshTrace. refresh_checks (RefreshCheckCollection, optional) : Defaults to RefreshCheckCollection(). default_row_count_check (bool, optional) : Defaults to True. refresh_type (RefreshType, optional) : Defaults to RefreshType.Full. ._refresh_partition source ._refresh_partition( partition: PyPartition ) Runs .Net RequestRefresh() on partition. RefreshCheck source RefreshCheck( name: str, function, assertion = None ) RefreshCheck is an test you run after your refreshes. It will run the given function before and after refreshes, then run the assertion of before and after. The default given in a refresh is to check row count. It will check row count before, and row count after. Then fail if row count after is zero. Methods: . init source .__init__( name: str, function, assertion = None ) Sets the necessary components to perform a refresh check. Args name (str) : Name of refresh check. function (Callable) : Function to run on pre and post checks. For example, a dax query. readme has examples of this. assertion (Callable, optional) : A function that can be run. Supply the assertion function with 2 arguments. The first one for your 'pre' results from the function argument. The second for your post results from the function argument. Return True or False depending on the comparison of the two arguments to determine a pass or fail status of your refresh. Defaults to None. .assertion_run source .assertion_run() Runs the given self.assertion function with self.pre and self.post . So, self.assertion_run(self.pre, self.post) . .pre_check source .pre_check() Runs self._check(\"Pre\") . .post_check source .post_check() Runs self._check(\"Post\") then self.assertion_run() . RefreshCheckCollection source RefreshCheckCollection( refresh_checks: RefreshCheck = [] ) Groups together your RefreshChecks . Used to handle multiple types of checks in a single refresh. Methods: .add_refresh_check source .add_refresh_check( refresh_check: RefreshCheck ) Add a RefreshCheck. Supply the RefreshCheck to add. Args refresh_check (RefreshCheck) : RefreshCheck class. .remove_refresh_check source .remove_refresh_check( refresh_check: RefreshCheck ) Remove a RefreshCheck. Supply the RefreshCheck to remove. Args refresh_check (RefreshCheck) : RefreshCheck class. .clear_refresh_checks source .clear_refresh_checks() Clear Refresh Checks.","title":"Refreshing"},{"location":"Refreshes/#_1","text":"","title":""},{"location":"Refreshes/#pyrefresh","text":"source PyRefresh( model, object: Union[str, PyTable, PyPartition, Dict[str, Any]], trace: BaseTrace = RefreshTrace, refresh_checks: RefreshCheckCollection = RefreshCheckCollection(), default_row_count_check: bool = True, refresh_type: RefreshType = RefreshType.Full ) PyRefresh Class to handle refreshes of model. Methods:","title":"PyRefresh"},{"location":"Refreshes/#_get_trace","text":"source ._get_trace() Creates Trace and creates it in model.","title":"._get_trace"},{"location":"Refreshes/#_find_partition","text":"source ._find_partition( table: Table, partition_str: str ) Finds partition in PyPartitions class.","title":"._find_partition"},{"location":"Refreshes/#_refresh_table","text":"source ._refresh_table( table: PyTable ) Runs .Net RequestRefresh() on table.","title":"._refresh_table"},{"location":"Refreshes/#_refresh_dict","text":"source ._refresh_dict( partition_dict: Dict ) Handles refreshes if argument given was a dictionary.","title":"._refresh_dict"},{"location":"Refreshes/#_find_table","text":"source ._find_table( table_str: str ) Finds table in PyTables class.","title":"._find_table"},{"location":"Refreshes/#_request_refresh","text":"source ._request_refresh( object ) Base method to parse through argument and figure out what needs to be refreshed. Someone please make this better...","title":"._request_refresh"},{"location":"Refreshes/#_pre_checks","text":"source ._pre_checks() Checks if any BaseTrace classes are needed from tabular_tracing.py . Then checks if any RefreshChecks are needed, along with the default row_count check.","title":"._pre_checks"},{"location":"Refreshes/#_post_checks","text":"source ._post_checks() If traces are running it Stops and Drops it. Runs through any post_checks() in RefreshChecks .","title":"._post_checks"},{"location":"Refreshes/#run","text":"source .run() When ready, execute to start the refresh process. First checks if connected and reconnects if needed. Then starts the trace if needed. Next will execute save_changes() and run the post checks after that. Last will return a pd.DataFrame of refresh results.","title":".run"},{"location":"Refreshes/#init","text":"source .__init__( model, object: Union[str, PyTable, PyPartition, Dict[str, Any]], trace: BaseTrace = RefreshTrace, refresh_checks: RefreshCheckCollection = RefreshCheckCollection(), default_row_count_check: bool = True, refresh_type: RefreshType = RefreshType.Full ) Init when a refresh is requested. Runs through requested tables and partitions to make sure they are in model. Then will run pre checks on the requested objects. Args model (Tabular) : Tabular model. object (Union[str, PyTable, PyPartition, Dict[str, Any]]) : The objects that you are wanting to refresh. Can be a PyTable , PyPartition , TABLE_NAME string, or a dict with {TABLE_REFERENCE:PARTITION_REFERENCE} trace (BaseTrace, optional) : Defaults to RefreshTrace. refresh_checks (RefreshCheckCollection, optional) : Defaults to RefreshCheckCollection(). default_row_count_check (bool, optional) : Defaults to True. refresh_type (RefreshType, optional) : Defaults to RefreshType.Full.","title":".init"},{"location":"Refreshes/#_refresh_partition","text":"source ._refresh_partition( partition: PyPartition ) Runs .Net RequestRefresh() on partition.","title":"._refresh_partition"},{"location":"Refreshes/#refreshcheck","text":"source RefreshCheck( name: str, function, assertion = None ) RefreshCheck is an test you run after your refreshes. It will run the given function before and after refreshes, then run the assertion of before and after. The default given in a refresh is to check row count. It will check row count before, and row count after. Then fail if row count after is zero. Methods:","title":"RefreshCheck"},{"location":"Refreshes/#init_1","text":"source .__init__( name: str, function, assertion = None ) Sets the necessary components to perform a refresh check. Args name (str) : Name of refresh check. function (Callable) : Function to run on pre and post checks. For example, a dax query. readme has examples of this. assertion (Callable, optional) : A function that can be run. Supply the assertion function with 2 arguments. The first one for your 'pre' results from the function argument. The second for your post results from the function argument. Return True or False depending on the comparison of the two arguments to determine a pass or fail status of your refresh. Defaults to None.","title":".init"},{"location":"Refreshes/#assertion_run","text":"source .assertion_run() Runs the given self.assertion function with self.pre and self.post . So, self.assertion_run(self.pre, self.post) .","title":".assertion_run"},{"location":"Refreshes/#pre_check","text":"source .pre_check() Runs self._check(\"Pre\") .","title":".pre_check"},{"location":"Refreshes/#post_check","text":"source .post_check() Runs self._check(\"Post\") then self.assertion_run() .","title":".post_check"},{"location":"Refreshes/#refreshcheckcollection","text":"source RefreshCheckCollection( refresh_checks: RefreshCheck = [] ) Groups together your RefreshChecks . Used to handle multiple types of checks in a single refresh. Methods:","title":"RefreshCheckCollection"},{"location":"Refreshes/#add_refresh_check","text":"source .add_refresh_check( refresh_check: RefreshCheck ) Add a RefreshCheck. Supply the RefreshCheck to add. Args refresh_check (RefreshCheck) : RefreshCheck class.","title":".add_refresh_check"},{"location":"Refreshes/#remove_refresh_check","text":"source .remove_refresh_check( refresh_check: RefreshCheck ) Remove a RefreshCheck. Supply the RefreshCheck to remove. Args refresh_check (RefreshCheck) : RefreshCheck class.","title":".remove_refresh_check"},{"location":"Refreshes/#clear_refresh_checks","text":"source .clear_refresh_checks() Clear Refresh Checks.","title":".clear_refresh_checks"},{"location":"Table/","text":"PyTable source PyTable( object, model ) The main PyTable class to interact with the tables in model. Notice the PyObject magic method __getattr__() will search in self._object if it is unable to find it in the default attributes. This let's you also easily check the default .Net properties. Methods: .related source .related() Returns tables with a relationship with the table in question. .refresh source .refresh( *args, **kwargs ) Use this to refresh the PyTable in question. You can pass through any extra parameters. For example: Tabular().Tables['Table Name'].Refresh(trace = None) Returns DataFrame : Returns pandas dataframe with some refresh details. .row_count source .row_count() Method to return count of rows. Simple Dax Query: EVALUATE {COUNTROWS('Table Name')} . Returns int : Number of rows using COUNTROWS . . init source .__init__( object, model ) Init extends from PyObject class. Also adds a few specific rows to the rich table. Args object (Table) : The actual .Net table. model (Tabular) : The model that the table is in. .last_refresh source .last_refresh() Will query each partition for the last refresh time. Then will select the max value to return. Returns datetime : Last refresh time in datetime format PyTables source PyTables( objects ) Groups together multiple tables. See PyObjects class for what more it can do. You can interact with PyTables straight from model. For ex: model.Tables . You can even filter down with .Find() . For example find all tables with fact in name. model.Tables.Find('fact') . Methods: .query_all source .query_all( query_function: str = 'COUNTROWS(_)' ) Dynamically query all tables. It will replace the _ with the query_function arg to build out the query to run. Args query_function (str, optional) : Dax query is dynamically building a query with the UNION & ROW DAX Functions. Defaults to 'COUNTROWS(_)'. Returns DataFrame : Returns dataframe with results .find_zero_rows source .find_zero_rows() Returns PyTables class of tables with zero rows queried. .refresh source .refresh( *args, **kwargs ) Refreshes all PyTable (s) in class. . init source .__init__( objects ) Init just extends from the main PyObjects class. .last_refresh source .last_refresh( group_partition: bool = True ) Returns pd.DataFrame of tables with their latest refresh time. Optional 'group_partition' variable, default is True. If False an extra column will be include to have the last refresh time to the grain of the partition Example to add to model model.Create_Table(p.Table_Last_Refresh_Times(model),'RefreshTimes') . Args model (pytabular.Tabular) : Tabular Model group_partition (bool, optional) : Whether or not you want the grain of the dataframe to be by table or by partition. Defaults to True. Returns DataFrame : pd dataframe with the RefreshedTime property If group_partition == True and the table has multiple partitions, then df.groupby(by[\"tables\"]).max()","title":"Tables"},{"location":"Table/#_1","text":"","title":""},{"location":"Table/#pytable","text":"source PyTable( object, model ) The main PyTable class to interact with the tables in model. Notice the PyObject magic method __getattr__() will search in self._object if it is unable to find it in the default attributes. This let's you also easily check the default .Net properties. Methods:","title":"PyTable"},{"location":"Table/#related","text":"source .related() Returns tables with a relationship with the table in question.","title":".related"},{"location":"Table/#refresh","text":"source .refresh( *args, **kwargs ) Use this to refresh the PyTable in question. You can pass through any extra parameters. For example: Tabular().Tables['Table Name'].Refresh(trace = None) Returns DataFrame : Returns pandas dataframe with some refresh details.","title":".refresh"},{"location":"Table/#row_count","text":"source .row_count() Method to return count of rows. Simple Dax Query: EVALUATE {COUNTROWS('Table Name')} . Returns int : Number of rows using COUNTROWS .","title":".row_count"},{"location":"Table/#init","text":"source .__init__( object, model ) Init extends from PyObject class. Also adds a few specific rows to the rich table. Args object (Table) : The actual .Net table. model (Tabular) : The model that the table is in.","title":".init"},{"location":"Table/#last_refresh","text":"source .last_refresh() Will query each partition for the last refresh time. Then will select the max value to return. Returns datetime : Last refresh time in datetime format","title":".last_refresh"},{"location":"Table/#pytables","text":"source PyTables( objects ) Groups together multiple tables. See PyObjects class for what more it can do. You can interact with PyTables straight from model. For ex: model.Tables . You can even filter down with .Find() . For example find all tables with fact in name. model.Tables.Find('fact') . Methods:","title":"PyTables"},{"location":"Table/#query_all","text":"source .query_all( query_function: str = 'COUNTROWS(_)' ) Dynamically query all tables. It will replace the _ with the query_function arg to build out the query to run. Args query_function (str, optional) : Dax query is dynamically building a query with the UNION & ROW DAX Functions. Defaults to 'COUNTROWS(_)'. Returns DataFrame : Returns dataframe with results","title":".query_all"},{"location":"Table/#find_zero_rows","text":"source .find_zero_rows() Returns PyTables class of tables with zero rows queried.","title":".find_zero_rows"},{"location":"Table/#refresh_1","text":"source .refresh( *args, **kwargs ) Refreshes all PyTable (s) in class.","title":".refresh"},{"location":"Table/#init_1","text":"source .__init__( objects ) Init just extends from the main PyObjects class.","title":".init"},{"location":"Table/#last_refresh_1","text":"source .last_refresh( group_partition: bool = True ) Returns pd.DataFrame of tables with their latest refresh time. Optional 'group_partition' variable, default is True. If False an extra column will be include to have the last refresh time to the grain of the partition Example to add to model model.Create_Table(p.Table_Last_Refresh_Times(model),'RefreshTimes') . Args model (pytabular.Tabular) : Tabular Model group_partition (bool, optional) : Whether or not you want the grain of the dataframe to be by table or by partition. Defaults to True. Returns DataFrame : pd dataframe with the RefreshedTime property If group_partition == True and the table has multiple partitions, then df.groupby(by[\"tables\"]).max()","title":".last_refresh"},{"location":"Tabular%20Editor%202/","text":"TabularEditor source TabularEditor( exe_file_path: str = 'Default' ) Setting Tabular_Editor Class for future work. Mainly runs download_tabular_editor() download_tabular_editor source .download_tabular_editor( download_location: str = 'https: //github.com/TabularEditor/TabularEditor/releases/download/2.16.7/TabularEditor.Portable.zip', folder: str = 'Tabular_Editor_2', auto_remove = True ) Runs a request.get() to retrieve the zip file from web. Will unzip response and store in directory. Will also register the removal of the new directory and files when exiting program. Args download_location (str, optional) : File path for zip of Tabular Editor 2. See code args for default download url. folder (str, optional) : New Folder Location. Defaults to \"Tabular_Editor_2\". auto_remove (bool, optional) : Boolean to determine auto removal of files once script exits. Defaults to True. Returns str : File path of TabularEditor.exe","title":"Tabular Editor"},{"location":"Tabular%20Editor%202/#_1","text":"","title":""},{"location":"Tabular%20Editor%202/#tabulareditor","text":"source TabularEditor( exe_file_path: str = 'Default' ) Setting Tabular_Editor Class for future work. Mainly runs download_tabular_editor()","title":"TabularEditor"},{"location":"Tabular%20Editor%202/#download_tabular_editor","text":"source .download_tabular_editor( download_location: str = 'https: //github.com/TabularEditor/TabularEditor/releases/download/2.16.7/TabularEditor.Portable.zip', folder: str = 'Tabular_Editor_2', auto_remove = True ) Runs a request.get() to retrieve the zip file from web. Will unzip response and store in directory. Will also register the removal of the new directory and files when exiting program. Args download_location (str, optional) : File path for zip of Tabular Editor 2. See code args for default download url. folder (str, optional) : New Folder Location. Defaults to \"Tabular_Editor_2\". auto_remove (bool, optional) : Boolean to determine auto removal of files once script exits. Defaults to True. Returns str : File path of TabularEditor.exe","title":"download_tabular_editor"},{"location":"Tabular/","text":"Tabular source Tabular( connection_str: str ) Tabular Class to perform operations. This is the main class to work with in PyTabular. You can connect to the other classes via the supplied attributes. Args connection_str (str) : Need a valid connection string: link Attributes AdomdConnection (Connection) : For querying. This is the Connection class. Tables (PyTables[PyTable]) : See PyTables for more information. Iterate through your tables in your model. Columns (PyColumns[PyColumn]) : See PyColumns for more information. Partitions (PyPartitions[PyPartition]) : See PyPartitions for more information. Measures (PyMeasures[PyMeasure]) : See PyMeasures for more information. Methods: .reload_model_info source .reload_model_info() Reload your model info into the Tabular class. Should be called after any model changes. Called in save_changes() and __init__() . Returns bool : True if successful .is_process source .is_process() Run method to check if Processing is occurring. Will query DMV $SYSTEM.DISCOVER_JOBS to see if any processing is happening. Returns bool : True if DMV shows Process, False if not. .disconnect source .disconnect() Disconnects from Model. .reconnect source .reconnect() Reconnects to Model. .refresh source .refresh( *args, **kwargs ) PyRefresh class to handle refreshes of model. See the PyRefresh() class for more details on what you can do with this. .save_changes source .save_changes() Called after refreshes or any model changes. Currently will return a named tuple of all changes detected. A ton of room for improvement on what gets returned here. .backup_table source .backup_table( table_str: str ) Will be removed. Used in conjunction with revert_table() . .revert_table source .revert_table( table_str: str ) Will be removed. This is used in conjunction with backup_table() . .query source .query( query_str: str, effective_user: str = None ) Executes query on model. See Connection().query() for details on execution. Args query_str (str) : Query string to execute. effective_user (str, optional) : Pass through an effective user if desired. It will create and store a new Connection() class if need, which will help with speed if looping through multiple users in a row. Defaults to None. Returns description .analyze_bpa source .analyze_bpa( tabular_editor_exe: str, best_practice_analyzer: str ) Takes your Tabular Model and performs TE2s BPA. Runs through Command line. Nothing fancy hear. Really just a simple wrapper so you could call BPA in the same python script. Args tabular_editor_exe (str) : TE2 Exe File path. Feel free to use class TE2().EXE_Path or provide your own. best_practice_analyzer (str) : BPA json file path. Feel free to use class BPA().Location or provide your own. Returns Assuming no failure, will return list of BPA violations. Else will return error from command line. .create_table source .create_table( df: pd.DataFrame, table_name: str ) Creates table from pd.DataFrame to a table in your model. It will convert the dataframe to M-Partition logic via the M query table constructor. Then will run a refresh and update model. Has some obvious limitations right now, because the datframe values are hard coded into M-Partition, which means you could hit limits with the size of your table. Args df (pd.DataFrame) : DataFrame to add to model. table_name (str) : Name of the table. Returns bool : True if successful","title":"Main Tabular Class"},{"location":"Tabular/#_1","text":"","title":""},{"location":"Tabular/#tabular","text":"source Tabular( connection_str: str ) Tabular Class to perform operations. This is the main class to work with in PyTabular. You can connect to the other classes via the supplied attributes. Args connection_str (str) : Need a valid connection string: link Attributes AdomdConnection (Connection) : For querying. This is the Connection class. Tables (PyTables[PyTable]) : See PyTables for more information. Iterate through your tables in your model. Columns (PyColumns[PyColumn]) : See PyColumns for more information. Partitions (PyPartitions[PyPartition]) : See PyPartitions for more information. Measures (PyMeasures[PyMeasure]) : See PyMeasures for more information. Methods:","title":"Tabular"},{"location":"Tabular/#reload_model_info","text":"source .reload_model_info() Reload your model info into the Tabular class. Should be called after any model changes. Called in save_changes() and __init__() . Returns bool : True if successful","title":".reload_model_info"},{"location":"Tabular/#is_process","text":"source .is_process() Run method to check if Processing is occurring. Will query DMV $SYSTEM.DISCOVER_JOBS to see if any processing is happening. Returns bool : True if DMV shows Process, False if not.","title":".is_process"},{"location":"Tabular/#disconnect","text":"source .disconnect() Disconnects from Model.","title":".disconnect"},{"location":"Tabular/#reconnect","text":"source .reconnect() Reconnects to Model.","title":".reconnect"},{"location":"Tabular/#refresh","text":"source .refresh( *args, **kwargs ) PyRefresh class to handle refreshes of model. See the PyRefresh() class for more details on what you can do with this.","title":".refresh"},{"location":"Tabular/#save_changes","text":"source .save_changes() Called after refreshes or any model changes. Currently will return a named tuple of all changes detected. A ton of room for improvement on what gets returned here.","title":".save_changes"},{"location":"Tabular/#backup_table","text":"source .backup_table( table_str: str ) Will be removed. Used in conjunction with revert_table() .","title":".backup_table"},{"location":"Tabular/#revert_table","text":"source .revert_table( table_str: str ) Will be removed. This is used in conjunction with backup_table() .","title":".revert_table"},{"location":"Tabular/#query","text":"source .query( query_str: str, effective_user: str = None ) Executes query on model. See Connection().query() for details on execution. Args query_str (str) : Query string to execute. effective_user (str, optional) : Pass through an effective user if desired. It will create and store a new Connection() class if need, which will help with speed if looping through multiple users in a row. Defaults to None. Returns description","title":".query"},{"location":"Tabular/#analyze_bpa","text":"source .analyze_bpa( tabular_editor_exe: str, best_practice_analyzer: str ) Takes your Tabular Model and performs TE2s BPA. Runs through Command line. Nothing fancy hear. Really just a simple wrapper so you could call BPA in the same python script. Args tabular_editor_exe (str) : TE2 Exe File path. Feel free to use class TE2().EXE_Path or provide your own. best_practice_analyzer (str) : BPA json file path. Feel free to use class BPA().Location or provide your own. Returns Assuming no failure, will return list of BPA violations. Else will return error from command line.","title":".analyze_bpa"},{"location":"Tabular/#create_table","text":"source .create_table( df: pd.DataFrame, table_name: str ) Creates table from pd.DataFrame to a table in your model. It will convert the dataframe to M-Partition logic via the M query table constructor. Then will run a refresh and update model. Has some obvious limitations right now, because the datframe values are hard coded into M-Partition, which means you could hit limits with the size of your table. Args df (pd.DataFrame) : DataFrame to add to model. table_name (str) : Name of the table. Returns bool : True if successful","title":".create_table"},{"location":"Traces/","text":"BaseTrace source BaseTrace( tabular_class, trace_events: List[TraceEvent], trace_event_columns: List[TraceColumn], handler: Callable ) Generates trace to be run on Server. This is the base class to customize the type of Trace you are looking for. It's recommended to use the out of the box traces built. It's on the roadmap to have an intuitive way to build traces for users. Methods: .start source .start() Call when you want to start the trace. Returns None : Returns None. Unless unsuccessful then it will return the error from Server. .build source .build() Run on init. This will take the inputed arguments for the class and attempt to build the Trace. Returns bool : True if successful .add source .add() Runs on init. Adds built trace to the Server. Returns int : Return int of placement in Server.Traces.get_Item(int). .stop source .stop() Call when you want to stop the trace. Returns None : Returns None. Unless unsuccessful then it will return the error from Server. .update source .update() Runs on init. Syncs with Server. Returns None : Returns None. Unless unsuccessful then it will return the error from Server. . init source .__init__( tabular_class, trace_events: List[TraceEvent], trace_event_columns: List[TraceColumn], handler: Callable ) This will build() , add() , and update() the trace to model. It will also register the dropping on the trace on exiting python. Args tabular_class (Tabular) : The model you want the trace for. trace_events (List[TraceEvent]) : The TraceEvents you want have in your trace. From Microsoft.AnalysisServices.TraceEventClass. trace_event_columns (List[TraceColumn]) : The trace event columns you want in your trace. From Microsoft.AnalysisServices.TraceColumn. handler (Callable) : The handler is a function that will take in two args. The first arg is source and it is currently unused. The second is arg is args and here is where you can access the results of the trace. .drop source .drop() Call when you want to drop the trace. Returns None : Returns None. Unless unsuccessful, then it will return the error from Server. RefreshTrace source RefreshTrace( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.ProgressReportBegin, TraceEventClass.ProgressReportCurrent, TraceEventClass.ProgressReportEnd, TraceEventClass.ProgressReportError], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.CurrentTime, TraceColumn.ObjectName, TraceColumn.ObjectPath, TraceColumn.DatabaseName, TraceColumn.SessionID, TraceColumn.TextData, TraceColumn.EventClass, TraceColumn.ProgressTotal], handler: Callable = _refresh_handler ) Subclass of BaseTrace() . Usefull for monitoring refreshes. This is the default trace that is run on refreshes. It will output all the various details into logger() . See _refresh_handler() for more details on what gets put into logger() . Methods: . init source .__init__( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.ProgressReportBegin, TraceEventClass.ProgressReportCurrent, TraceEventClass.ProgressReportEnd, TraceEventClass.ProgressReportError], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.CurrentTime, TraceColumn.ObjectName, TraceColumn.ObjectPath, TraceColumn.DatabaseName, TraceColumn.SessionID, TraceColumn.TextData, TraceColumn.EventClass, TraceColumn.ProgressTotal], handler: Callable = _refresh_handler ) Init will extend through BaseTrace() . But pass through specific params. Args tabular_class (Tabular) : This is your Tabular() class. trace_events (List[TraceEvent], optional) : Defaults to [ TraceEventClass.ProgressReportBegin, TraceEventClass.ProgressReportCurrent, TraceEventClass.ProgressReportEnd, TraceEventClass.ProgressReportError, ]. trace_event_columns (List[TraceColumn], optional) : Defaults to [ TraceColumn.EventSubclass, TraceColumn.CurrentTime, TraceColumn.ObjectName, TraceColumn.ObjectPath, TraceColumn.DatabaseName, TraceColumn.SessionID, TraceColumn.TextData, TraceColumn.EventClass, TraceColumn.ProgressTotal, ]. handler (Callable, optional) : description . Defaults to _refresh_handler. QueryMonitor source QueryMonitor( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.QueryEnd], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.StartTime, TraceColumn.EndTime, TraceColumn.Duration, TraceColumn.Severity, TraceColumn.Error, TraceColumn.NTUserName, TraceColumn.DatabaseName, TraceColumn.ApplicationName, TraceColumn.TextData], handler: Callable = _query_monitor_handler ) Subclass of BaseTrace() . Usefull for monitoring queries. The default handler for QueryMonitor() shows full query in logger.debug() . So you will need to set your logger to debug() if you would like to see them. Otherwise, will show basic info on who/what is querying. Methods: . init source .__init__( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.QueryEnd], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.StartTime, TraceColumn.EndTime, TraceColumn.Duration, TraceColumn.Severity, TraceColumn.Error, TraceColumn.NTUserName, TraceColumn.DatabaseName, TraceColumn.ApplicationName, TraceColumn.TextData], handler: Callable = _query_monitor_handler ) Init will extend through to BaseTrace, but pass through specific params. Args tabular_class (Tabular) : This is your Tabular() class. All that will need to provided to successfully init. trace_events (List[TraceEvent], optional) : Defaults to [TraceEventClass.QueryEnd]. trace_event_columns (List[TraceColumn], optional) : Defaults to [ TraceColumn.EventSubclass, TraceColumn.StartTime, TraceColumn.EndTime, TraceColumn.Duration, TraceColumn.Severity, TraceColumn.Error, TraceColumn.NTUserName, TraceColumn.DatabaseName, TraceColumn.ApplicationName, TraceColumn.TextData, ]. handler (Callable, optional) : Defaults to _query_monitor_handler() . _refresh_handler source ._refresh_handler( source, args ) Default handler called when RefreshTrace() is used. It will log various steps of the refresh process. Mostly will output the current # of rows read. Will output logger.warning() if refresh produces zero rows, or if a switching dictionary event occurs. The rest of the EventSubclass' will output the raw text. For example, TabularSequencePoint, TabularRefresh, Process, Vertipaq, CompressSegment, TabularCommit, RelationshipBuildPrepare, AnalyzeEncodeData, ReadData. If there is anything else not prebuilt out for logging, it will dump the arguments int logger.debug() . _query_monitor_handler source ._query_monitor_handler( source, args ) Default function used with the Query_Monitor() trace. Will return query type, user (effective user), application, start time, end time, and total seconds of query in logger.info() . To see full query set logger to debug logger.setLevel(logging.DEBUG) .","title":"Traces"},{"location":"Traces/#_1","text":"","title":""},{"location":"Traces/#basetrace","text":"source BaseTrace( tabular_class, trace_events: List[TraceEvent], trace_event_columns: List[TraceColumn], handler: Callable ) Generates trace to be run on Server. This is the base class to customize the type of Trace you are looking for. It's recommended to use the out of the box traces built. It's on the roadmap to have an intuitive way to build traces for users. Methods:","title":"BaseTrace"},{"location":"Traces/#start","text":"source .start() Call when you want to start the trace. Returns None : Returns None. Unless unsuccessful then it will return the error from Server.","title":".start"},{"location":"Traces/#build","text":"source .build() Run on init. This will take the inputed arguments for the class and attempt to build the Trace. Returns bool : True if successful","title":".build"},{"location":"Traces/#add","text":"source .add() Runs on init. Adds built trace to the Server. Returns int : Return int of placement in Server.Traces.get_Item(int).","title":".add"},{"location":"Traces/#stop","text":"source .stop() Call when you want to stop the trace. Returns None : Returns None. Unless unsuccessful then it will return the error from Server.","title":".stop"},{"location":"Traces/#update","text":"source .update() Runs on init. Syncs with Server. Returns None : Returns None. Unless unsuccessful then it will return the error from Server.","title":".update"},{"location":"Traces/#init","text":"source .__init__( tabular_class, trace_events: List[TraceEvent], trace_event_columns: List[TraceColumn], handler: Callable ) This will build() , add() , and update() the trace to model. It will also register the dropping on the trace on exiting python. Args tabular_class (Tabular) : The model you want the trace for. trace_events (List[TraceEvent]) : The TraceEvents you want have in your trace. From Microsoft.AnalysisServices.TraceEventClass. trace_event_columns (List[TraceColumn]) : The trace event columns you want in your trace. From Microsoft.AnalysisServices.TraceColumn. handler (Callable) : The handler is a function that will take in two args. The first arg is source and it is currently unused. The second is arg is args and here is where you can access the results of the trace.","title":".init"},{"location":"Traces/#drop","text":"source .drop() Call when you want to drop the trace. Returns None : Returns None. Unless unsuccessful, then it will return the error from Server.","title":".drop"},{"location":"Traces/#refreshtrace","text":"source RefreshTrace( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.ProgressReportBegin, TraceEventClass.ProgressReportCurrent, TraceEventClass.ProgressReportEnd, TraceEventClass.ProgressReportError], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.CurrentTime, TraceColumn.ObjectName, TraceColumn.ObjectPath, TraceColumn.DatabaseName, TraceColumn.SessionID, TraceColumn.TextData, TraceColumn.EventClass, TraceColumn.ProgressTotal], handler: Callable = _refresh_handler ) Subclass of BaseTrace() . Usefull for monitoring refreshes. This is the default trace that is run on refreshes. It will output all the various details into logger() . See _refresh_handler() for more details on what gets put into logger() . Methods:","title":"RefreshTrace"},{"location":"Traces/#init_1","text":"source .__init__( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.ProgressReportBegin, TraceEventClass.ProgressReportCurrent, TraceEventClass.ProgressReportEnd, TraceEventClass.ProgressReportError], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.CurrentTime, TraceColumn.ObjectName, TraceColumn.ObjectPath, TraceColumn.DatabaseName, TraceColumn.SessionID, TraceColumn.TextData, TraceColumn.EventClass, TraceColumn.ProgressTotal], handler: Callable = _refresh_handler ) Init will extend through BaseTrace() . But pass through specific params. Args tabular_class (Tabular) : This is your Tabular() class. trace_events (List[TraceEvent], optional) : Defaults to [ TraceEventClass.ProgressReportBegin, TraceEventClass.ProgressReportCurrent, TraceEventClass.ProgressReportEnd, TraceEventClass.ProgressReportError, ]. trace_event_columns (List[TraceColumn], optional) : Defaults to [ TraceColumn.EventSubclass, TraceColumn.CurrentTime, TraceColumn.ObjectName, TraceColumn.ObjectPath, TraceColumn.DatabaseName, TraceColumn.SessionID, TraceColumn.TextData, TraceColumn.EventClass, TraceColumn.ProgressTotal, ]. handler (Callable, optional) : description . Defaults to _refresh_handler.","title":".init"},{"location":"Traces/#querymonitor","text":"source QueryMonitor( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.QueryEnd], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.StartTime, TraceColumn.EndTime, TraceColumn.Duration, TraceColumn.Severity, TraceColumn.Error, TraceColumn.NTUserName, TraceColumn.DatabaseName, TraceColumn.ApplicationName, TraceColumn.TextData], handler: Callable = _query_monitor_handler ) Subclass of BaseTrace() . Usefull for monitoring queries. The default handler for QueryMonitor() shows full query in logger.debug() . So you will need to set your logger to debug() if you would like to see them. Otherwise, will show basic info on who/what is querying. Methods:","title":"QueryMonitor"},{"location":"Traces/#init_2","text":"source .__init__( tabular_class, trace_events: List[TraceEvent] = [TraceEventClass.QueryEnd], trace_event_columns: List[TraceColumn] = [TraceColumn.EventSubclass, TraceColumn.StartTime, TraceColumn.EndTime, TraceColumn.Duration, TraceColumn.Severity, TraceColumn.Error, TraceColumn.NTUserName, TraceColumn.DatabaseName, TraceColumn.ApplicationName, TraceColumn.TextData], handler: Callable = _query_monitor_handler ) Init will extend through to BaseTrace, but pass through specific params. Args tabular_class (Tabular) : This is your Tabular() class. All that will need to provided to successfully init. trace_events (List[TraceEvent], optional) : Defaults to [TraceEventClass.QueryEnd]. trace_event_columns (List[TraceColumn], optional) : Defaults to [ TraceColumn.EventSubclass, TraceColumn.StartTime, TraceColumn.EndTime, TraceColumn.Duration, TraceColumn.Severity, TraceColumn.Error, TraceColumn.NTUserName, TraceColumn.DatabaseName, TraceColumn.ApplicationName, TraceColumn.TextData, ]. handler (Callable, optional) : Defaults to _query_monitor_handler() .","title":".init"},{"location":"Traces/#_refresh_handler","text":"source ._refresh_handler( source, args ) Default handler called when RefreshTrace() is used. It will log various steps of the refresh process. Mostly will output the current # of rows read. Will output logger.warning() if refresh produces zero rows, or if a switching dictionary event occurs. The rest of the EventSubclass' will output the raw text. For example, TabularSequencePoint, TabularRefresh, Process, Vertipaq, CompressSegment, TabularCommit, RelationshipBuildPrepare, AnalyzeEncodeData, ReadData. If there is anything else not prebuilt out for logging, it will dump the arguments int logger.debug() .","title":"_refresh_handler"},{"location":"Traces/#_query_monitor_handler","text":"source ._query_monitor_handler( source, args ) Default function used with the Query_Monitor() trace. Will return query type, user (effective user), application, start time, end time, and total seconds of query in logger.info() . To see full query set logger to debug logger.setLevel(logging.DEBUG) .","title":"_query_monitor_handler"},{"location":"contributing/","text":"Contributing Guidelines Goal Make Python a first class citizen for interacting with Tabular models . Some rules See github actions for checks run on pull request. We docstring-coverage to check for 100% docstring coverage. flake8 also runs, but with a few extra packages. (pep8-naming, flake8-docstrings). Updates of any kind are welcome! Even just letting me know of the issues. Or updating docstrings... Limit any extra packages, see pyproject.toml for dependencies Install pre-commit. Pre-commit will run pytest, flake8, and docstr-coverage before push. pip install pre-commit pre-commit install --hook-type pre-push This will take a while... pytest will open a PBIX file in repository and run tests on it... Eventually these tests will be run on a model that is not local. Documentation help Docstrings follow the google docstring convention. See Example . The flake8-docstrings will check that google docstring format is followed. Docstrings get converted to markdown with the mkgendocs package. Then gets converted to readthedocs site with the mkdocs package. Misc Work will be distributed under a MIT license.","title":"Contributing"},{"location":"contributing/#contributing-guidelines","text":"","title":"Contributing Guidelines"},{"location":"contributing/#goal","text":"Make Python a first class citizen for interacting with Tabular models .","title":"Goal"},{"location":"contributing/#some-rules","text":"See github actions for checks run on pull request. We docstring-coverage to check for 100% docstring coverage. flake8 also runs, but with a few extra packages. (pep8-naming, flake8-docstrings). Updates of any kind are welcome! Even just letting me know of the issues. Or updating docstrings... Limit any extra packages, see pyproject.toml for dependencies Install pre-commit. Pre-commit will run pytest, flake8, and docstr-coverage before push. pip install pre-commit pre-commit install --hook-type pre-push This will take a while... pytest will open a PBIX file in repository and run tests on it... Eventually these tests will be run on a model that is not local.","title":"Some rules"},{"location":"contributing/#documentation-help","text":"Docstrings follow the google docstring convention. See Example . The flake8-docstrings will check that google docstring format is followed. Docstrings get converted to markdown with the mkgendocs package. Then gets converted to readthedocs site with the mkdocs package.","title":"Documentation help"},{"location":"contributing/#misc","text":"Work will be distributed under a MIT license.","title":"Misc"}]}